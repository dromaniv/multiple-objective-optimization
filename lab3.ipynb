{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c83d287",
   "metadata": {},
   "source": [
    "#  Evolutionary Multiple-objective Optimization (part for 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed746013",
   "metadata": {},
   "source": [
    "- This script is for those who want to improve their final grade from 3.0 to 4.0. \n",
    "- Your task is to implement any one evolutionary algorithm for multiple-objective optimization introduced during the lecture (NSGA-II/NSGA-III/MOEA/D; except for NSGA).\n",
    "- Note that it has to be your implementation (using external libraries is forbidden; EXCEPTION: you can use the JECDM framework: https://jecdm.cs.put.poznan.pl -- but it is a relatively complex software, and much effort must be spent to understand how to use it).\n",
    "- The problem to be solved is the portfolio optimization tackled during lab 1.\n",
    "- You can use the same data and price predictions as you made for lab 1 (Bundle1.zip) or update them accordingly to the next stage if you participate in the portfolio game (it is up to you).\n",
    "- Apart from the two-objective scenario, tackle also a three-objective one. As for the third objective, think about some reasonable risk-measure. E.g., you can maximize the number of non-zero weights, which should refer to minimizing risk by diversifying investments.\n",
    "- Perform experimental evaluation of your implementation. You can use, e.g., the IDG or the HV metric to quantify the quality of populations constructed by the method.\n",
    "- The experimental evaluation should be \"reasonably extensive.\" E.g., run your method multiple times and average the results, show average convergence plots, do the sensitivity analysis (just four combinations of population size/generations will be enough), and depict some final populations. Also, compare the populations (only for 2D scenarios) with those generated by the ECM or WSM algorithm. Note that ECM and WSM already generate Pareto optimal solutions, so these can be considered good benchmarks for comparison.\n",
    "- You can report your results here, i.e., in the jupyter notebook. You do not need to prepare any pdf report, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe7410",
   "metadata": {},
   "source": [
    "# Lab 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644ad0b3",
   "metadata": {},
   "source": [
    "Import the necessary libraries (numpy, pandas, matplotlib, cvxopt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3363d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from cvxopt import matrix, solvers\n",
    "from sklearn.linear_model import Lasso\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# For reproducible randomness\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e074506",
   "metadata": {},
   "source": [
    "Load the data from *Part1.txt files. Each file’s first line is the asset name, second line is N, and then N lines with “time price”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d26dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_asset_data(data_folder=\"Bundle3\"):\n",
    "    asset_names = []\n",
    "    asset_times = []\n",
    "    asset_prices = []\n",
    "    \n",
    "    txt_files = [f for f in os.listdir(data_folder) if f.endswith(\"Part3.txt\")]\n",
    "    \n",
    "    for fname in txt_files:\n",
    "        path = os.path.join(data_folder, fname)\n",
    "        with open(path, \"r\") as f:\n",
    "            # 1) asset name\n",
    "            asset_name = f.readline().strip()\n",
    "    \n",
    "            # 2) number of data points\n",
    "            N_line = f.readline().strip()\n",
    "            N = int(N_line)\n",
    "    \n",
    "            # 3) read time, price lines\n",
    "            times = []\n",
    "            prices = []\n",
    "            for _ in range(N):\n",
    "                line = f.readline().strip()\n",
    "                t_str, p_str = line.split()\n",
    "                times.append(float(t_str))\n",
    "                prices.append(float(p_str))\n",
    "    \n",
    "            asset_names.append(asset_name)\n",
    "            asset_times.append(times)\n",
    "            asset_prices.append(prices)\n",
    "    \n",
    "    print(f\"Found {len(asset_names)} assets.\")\n",
    "    print(\"First few asset names:\", asset_names[:5])\n",
    "    return asset_names, asset_times, asset_prices\n",
    "\n",
    "asset_names, asset_times, asset_prices = load_asset_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192418ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stock_predictor import StockPredictor\n",
    "\n",
    "# Create an instance with your data folder - works with any bundle number\n",
    "# Examples: \"Bundle2\", \"Bundle3\", etc.\n",
    "sp = StockPredictor(data_folder=\"Bundle3\", verbose=False)\n",
    "\n",
    "# Generate and plot market-aware predictions that account for market cycles\n",
    "market_predictions = sp.plot_market_aware_predictions(\n",
    "    training_start=0,\n",
    "    training_end=None,  # Will default to the last available time point\n",
    "    forecast_time=None, # Will default to training_end + 100\n",
    "    show_raw=True      # Show both raw model and market-adjusted forecasts\n",
    ")\n",
    "\n",
    "# The market_predictions variable now contains the curve fit results for each asset\n",
    "# Example of accessing a specific asset's prediction:\n",
    "# asset_prediction = market_predictions[\"AssetName\"]\n",
    "# print(asset_prediction[\"model_name\"])         # The selected model\n",
    "# print(asset_prediction[\"predicted_return\"])   # The forecasted return percentage\n",
    "# print(asset_prediction[\"market_info\"])        # Market cycle information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0371b04",
   "metadata": {},
   "source": [
    "### Markowitz Model using **mean** results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb42fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract expected returns from mean results\n",
    "expected_returns = np.array([market_predictions[asset][\"predicted_return\"] / 100.0 for asset in asset_names])\n",
    "n_assets = len(asset_names)\n",
    "\n",
    "# Compute covariance matrix using historical data\n",
    "historical_returns = []\n",
    "for i, asset in enumerate(asset_names):\n",
    "    # Get prices during training period\n",
    "    prices = np.array(asset_prices[i])\n",
    "    times = np.array(asset_times[i])\n",
    "    mask = (times >= 0) & (times <= 100)\n",
    "    asset_prices_train = prices[mask]\n",
    "    \n",
    "    # Calculate returns\n",
    "    asset_returns = np.diff(asset_prices_train) / asset_prices_train[:-1]\n",
    "    historical_returns.append(asset_returns)\n",
    "\n",
    "# Ensure all historical returns have the same length\n",
    "min_length = min(len(returns) for returns in historical_returns)\n",
    "historical_returns = [returns[:min_length] for returns in historical_returns]\n",
    "historical_returns = np.array(historical_returns)\n",
    "\n",
    "# Compute covariance matrix\n",
    "cov_matrix = np.cov(historical_returns)\n",
    "\n",
    "# Ensure covariance matrix is positive semi-definite\n",
    "eigenvalues = np.linalg.eigvalsh(cov_matrix)\n",
    "if np.any(eigenvalues < 0):\n",
    "    cov_matrix += np.eye(n_assets) * 1e-8\n",
    "\n",
    "print(\"Expected Returns (sample):\")\n",
    "for i in range(min(5, n_assets)):\n",
    "    print(f\"{asset_names[i]}: {expected_returns[i]:.4f}\")\n",
    "\n",
    "print(\"\\nCovariance Matrix (sample):\")\n",
    "print(cov_matrix[:3, :3])\n",
    "\n",
    "# Find minimum risk portfolio\n",
    "def solve_min_risk():\n",
    "    \"\"\"Find the portfolio with minimum risk\"\"\"\n",
    "    P = matrix(cov_matrix)\n",
    "    q = matrix(np.zeros(n_assets))\n",
    "    \n",
    "    # Constraint: w ≥ 0\n",
    "    G = matrix(-np.eye(n_assets))\n",
    "    h = matrix(np.zeros(n_assets))\n",
    "    \n",
    "    # Constraint: sum(w) = 1\n",
    "    A = matrix(np.ones((1, n_assets)))\n",
    "    b = matrix(np.ones(1))\n",
    "    \n",
    "    sol = solvers.qp(P, q, G, h, A, b)\n",
    "    weights = np.array(sol['x']).flatten()\n",
    "    port_return = np.dot(weights, expected_returns)\n",
    "    port_risk = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "    \n",
    "    return weights, port_risk, port_return\n",
    "\n",
    "# Find maximum return portfolio\n",
    "def solve_max_return():\n",
    "    \"\"\"Find the portfolio with maximum return\"\"\"\n",
    "    c = matrix(-expected_returns)  # Negative because we minimize -r^T w\n",
    "    \n",
    "    # Constraint: w ≥ 0\n",
    "    G = matrix(-np.eye(n_assets))\n",
    "    h = matrix(np.zeros(n_assets))\n",
    "    \n",
    "    # Constraint: sum(w) = 1\n",
    "    A = matrix(np.ones((1, n_assets)))\n",
    "    b = matrix(np.ones(1))\n",
    "    \n",
    "    sol = solvers.lp(c, G, h, A, b)\n",
    "    weights = np.array(sol['x']).flatten()\n",
    "    port_return = np.dot(weights, expected_returns)\n",
    "    port_risk = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "    \n",
    "    return weights, port_return, port_risk\n",
    "\n",
    "# Find the extreme portfolios\n",
    "min_risk_weights, min_risk_value, min_risk_return = solve_min_risk()\n",
    "max_return_weights, max_return_value, max_return_risk = solve_max_return()\n",
    "\n",
    "print(\"\\nMinimum Risk Portfolio:\")\n",
    "print(f\"Risk: {min_risk_value:.6f}\")\n",
    "print(f\"Return: {min_risk_return:.6f}\")\n",
    "\n",
    "print(\"\\nMaximum Return Portfolio:\")\n",
    "print(f\"Return: {max_return_value:.6f}\")\n",
    "print(f\"Risk: {max_return_risk:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa4f5f3",
   "metadata": {},
   "source": [
    "### Implementation of the two multi-objective methods:\n",
    "- **Weighted Sum Method (WSM)**  \n",
    "- **Epsilon-Constraint Method (ECM)**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33350e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress solver output\n",
    "solvers.options['show_progress'] = False\n",
    "\n",
    "# Weighted Sum Method (WSM)\n",
    "def weighted_sum_method(n_weights=10, normalize=True):\n",
    "    # Set up normalization ranges if needed\n",
    "    if normalize:\n",
    "        return_range = max_return_value - min_risk_return\n",
    "        risk_range = max_return_risk - min_risk_value\n",
    "    \n",
    "    # Generate weight vectors\n",
    "    weight_vectors = []\n",
    "    for i in range(n_weights):\n",
    "        w_return = i / (n_weights - 1)  # Weight for return\n",
    "        w_risk = 1 - w_return           # Weight for risk\n",
    "        weight_vectors.append((w_return, w_risk))\n",
    "    \n",
    "    # Solve for each weight vector\n",
    "    solutions = []\n",
    "    for w_return, w_risk in weight_vectors:\n",
    "        # Setup quadratic programming parameters\n",
    "        P = matrix(cov_matrix)\n",
    "        q = matrix(np.zeros(n_assets))\n",
    "        \n",
    "        if normalize:\n",
    "            # For normalized objective function\n",
    "            q_mod = matrix(-w_return * expected_returns / return_range)\n",
    "            P_mod = matrix(w_risk * cov_matrix / risk_range)\n",
    "        else:\n",
    "            # For non-normalized objective function\n",
    "            q_mod = matrix(-w_return * expected_returns)\n",
    "            P_mod = matrix(w_risk * cov_matrix)\n",
    "        \n",
    "        # Constraint: w ≥ 0\n",
    "        G = matrix(-np.eye(n_assets))\n",
    "        h = matrix(np.zeros(n_assets))\n",
    "        \n",
    "        # Constraint: sum(w) = 1\n",
    "        A = matrix(np.ones((1, n_assets)))\n",
    "        b = matrix(np.ones(1))\n",
    "        \n",
    "        # Solve QP\n",
    "        sol = solvers.qp(P_mod, q_mod, G, h, A, b)\n",
    "        \n",
    "        if sol['status'] != 'optimal':\n",
    "            print(f\"Warning: Optimization did not reach an optimal solution for weights {w_return}, {w_risk}. Status: {sol['status']}\")\n",
    "            continue\n",
    "        \n",
    "        weights = np.array(sol['x']).flatten()\n",
    "        port_return = np.dot(weights, expected_returns)\n",
    "        port_risk = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "        \n",
    "        solutions.append((weights, port_return, port_risk))\n",
    "    \n",
    "    # Filter duplicate solutions\n",
    "    unique_solutions = []\n",
    "    for sol in solutions:\n",
    "        is_unique = True\n",
    "        for existing_sol in unique_solutions:\n",
    "            if (np.abs(sol[1] - existing_sol[1]) < 1e-6 and \n",
    "                np.abs(sol[2] - existing_sol[2]) < 1e-6):\n",
    "                is_unique = False\n",
    "                break\n",
    "        if is_unique:\n",
    "            unique_solutions.append(sol)\n",
    "    \n",
    "    return unique_solutions\n",
    "\n",
    "# Epsilon-Constraint Method (ECM)\n",
    "def epsilon_constraint_method(n_thresholds=10):\n",
    "    \"\"\"\n",
    "    Implements the Epsilon-Constraint Method for portfolio optimization.\n",
    "    \n",
    "    Args:\n",
    "        n_thresholds: Number of threshold values to use\n",
    "    \n",
    "    Returns:\n",
    "        List of Pareto optimal solutions (weights, return, risk)\n",
    "    \"\"\"\n",
    "    # Generate threshold values\n",
    "    thresholds = []\n",
    "    for i in range(n_thresholds):\n",
    "        t = min_risk_return + (i / (n_thresholds - 1)) * (max_return_value - min_risk_return)\n",
    "        thresholds.append(t)\n",
    "    \n",
    "    # Solve for each threshold\n",
    "    solutions = []\n",
    "    for threshold in thresholds:\n",
    "        # Setup quadratic programming parameters\n",
    "        P = matrix(cov_matrix)\n",
    "        q = matrix(np.zeros(n_assets))\n",
    "        \n",
    "        # Constraint: w ≥ 0\n",
    "        G = matrix(-np.eye(n_assets))\n",
    "        h = matrix(np.zeros(n_assets))\n",
    "        \n",
    "        # Constraints: sum(w) = 1 and r^T w >= threshold\n",
    "        A = matrix(np.vstack((\n",
    "            np.ones(n_assets),     # sum(w) = 1\n",
    "            expected_returns       # r^T w >= threshold\n",
    "        )))\n",
    "        b = matrix(np.array([1.0, threshold]))\n",
    "        \n",
    "        # Solve QP\n",
    "        sol = solvers.qp(P, q, G, h, A, b)\n",
    "        \n",
    "        if sol['status'] != 'optimal':\n",
    "            print(f\"Warning: Optimization did not reach an optimal solution for threshold {threshold}. Status: {sol['status']}\")\n",
    "            continue\n",
    "        \n",
    "        weights = np.array(sol['x']).flatten()\n",
    "        port_return = np.dot(weights, expected_returns)\n",
    "        port_risk = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "        \n",
    "        solutions.append((weights, port_return, port_risk))\n",
    "    \n",
    "    # Filter duplicate solutions\n",
    "    unique_solutions = []\n",
    "    for sol in solutions:\n",
    "        is_unique = True\n",
    "        for existing_sol in unique_solutions:\n",
    "            if (np.abs(sol[1] - existing_sol[1]) < 1e-6 and \n",
    "                np.abs(sol[2] - existing_sol[2]) < 1e-6):\n",
    "                is_unique = False\n",
    "                break\n",
    "        if is_unique:\n",
    "            unique_solutions.append(sol)\n",
    "    \n",
    "    return unique_solutions\n",
    "\n",
    "# Function to plot Pareto front\n",
    "def plot_pareto_front(solutions, title):\n",
    "    risks = [sol[2] for sol in solutions]\n",
    "    returns = [sol[1] for sol in solutions]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(risks, returns, c='red', marker='o')\n",
    "    plt.xlabel('Risk')\n",
    "    plt.ylabel('Expected Return')\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Test the methods with a small number of points\n",
    "wsm_solutions = weighted_sum_method(100)\n",
    "ecm_solutions = epsilon_constraint_method(10)\n",
    "\n",
    "print(f\"WSM found {len(wsm_solutions)} unique solutions\")\n",
    "print(f\"ECM found {len(ecm_solutions)} unique solutions\")\n",
    "\n",
    "plot_pareto_front(wsm_solutions, \"WSM Pareto Front\")\n",
    "plot_pareto_front(ecm_solutions, \"ECM Pareto Front\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65fb126",
   "metadata": {},
   "source": [
    "# Lab 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabdf22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "# Suppress solver output\n",
    "solvers.options['show_progress'] = False\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "class NSGA2:\n",
    "    def __init__(self, n_assets, expected_returns, cov_matrix, \n",
    "                 pop_size=100, max_gen=100, crossover_prob=0.9, mutation_prob=0.1,\n",
    "                 tournament_size=2, mutation_strength=0.1, n_objectives=2,\n",
    "                 adaptive_mutation=False, mutation_min=0.01, mutation_max=0.25,\n",
    "                 hybrid_optimization=False, hybrid_frequency=20):\n",
    "        self.n_assets = n_assets\n",
    "        self.expected_returns = expected_returns\n",
    "        self.cov_matrix = cov_matrix\n",
    "        self.pop_size = pop_size\n",
    "        self.max_gen = max_gen\n",
    "        self.crossover_prob = crossover_prob\n",
    "        self.mutation_prob = mutation_prob\n",
    "        self.tournament_size = tournament_size\n",
    "        self.mutation_strength = mutation_strength\n",
    "        self.n_objectives = n_objectives\n",
    "        \n",
    "        # Adaptive mutation parameters\n",
    "        self.adaptive_mutation = adaptive_mutation\n",
    "        self.mutation_min = mutation_min\n",
    "        self.mutation_max = mutation_max\n",
    "        self.current_mutation_rate = mutation_prob\n",
    "        self.convergence_window = 5  # Number of generations to check for convergence\n",
    "        self.convergence_threshold = 0.001  # Threshold for detecting convergence\n",
    "        \n",
    "        # Hybrid optimization parameters\n",
    "        self.hybrid_optimization = hybrid_optimization\n",
    "        self.hybrid_frequency = hybrid_frequency  # How often to apply QP refinement\n",
    "        \n",
    "        # For tracking metrics\n",
    "        self.hypervolume_history = []\n",
    "        self.igd_history = []\n",
    "        self.reference_point = None\n",
    "        self.diversity_history = []  # Track population diversity\n",
    "        self.mutation_rate_history = []\n",
    "    \n",
    "    def initialize_population(self):\n",
    "        population = []\n",
    "        \n",
    "        for _ in range(self.pop_size):\n",
    "            # Generate random weights\n",
    "            weights = np.random.random(self.n_assets)\n",
    "            weights /= np.sum(weights)  # Normalize to sum to 1\n",
    "            \n",
    "            # Evaluate objectives\n",
    "            objectives = self.evaluate_objectives(weights)\n",
    "            \n",
    "            # Create individual\n",
    "            individual = {\n",
    "                'weights': weights,\n",
    "                'objectives': objectives,\n",
    "                'rank': None,\n",
    "                'crowding_distance': None\n",
    "            }\n",
    "            \n",
    "            population.append(individual)\n",
    "        \n",
    "        return population\n",
    "    \n",
    "    def evaluate_objectives(self, weights):\n",
    "        # Expected return (we negate it because we're minimizing)\n",
    "        expected_return = -np.dot(weights, self.expected_returns)\n",
    "        \n",
    "        # Risk (standard deviation)\n",
    "        risk = np.sqrt(np.dot(weights.T, np.dot(self.cov_matrix, weights)))\n",
    "        \n",
    "        if self.n_objectives == 2:\n",
    "            return np.array([expected_return, risk])\n",
    "        \n",
    "        # Diversification measure (negative because we want to maximize)\n",
    "        # Count number of assets with significant weight (>1%)\n",
    "        diversification = -np.sum(weights > 0.01)\n",
    "        \n",
    "        return np.array([expected_return, risk, diversification])\n",
    "    \n",
    "    def dominates(self, ind1, ind2):\n",
    "        # ind1 dominates ind2 if:\n",
    "        # 1. ind1 is no worse than ind2 in all objectives\n",
    "        # 2. ind1 is strictly better than ind2 in at least one objective\n",
    "        \n",
    "        obj1 = ind1['objectives']\n",
    "        obj2 = ind2['objectives']\n",
    "        \n",
    "        no_worse = np.all(obj1 <= obj2)\n",
    "        strictly_better = np.any(obj1 < obj2)\n",
    "        \n",
    "        return no_worse and strictly_better\n",
    "    \n",
    "    def fast_non_dominated_sort(self, population):\n",
    "        # Initialize with first front\n",
    "        fronts = []\n",
    "        first_front = []\n",
    "        \n",
    "        # For each individual\n",
    "        for i, p in enumerate(population):\n",
    "            p['domination_count'] = 0  # Number of solutions that dominate p\n",
    "            p['dominated_solutions'] = []  # Set of solutions that p dominates\n",
    "            \n",
    "            for j, q in enumerate(population):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                    \n",
    "                if self.dominates(p, q):\n",
    "                    p['dominated_solutions'].append(j)\n",
    "                elif self.dominates(q, p):\n",
    "                    p['domination_count'] += 1\n",
    "            \n",
    "            if p['domination_count'] == 0:\n",
    "                p['rank'] = 0  # Rank of the first front\n",
    "                first_front.append(i)\n",
    "        \n",
    "        # Add first front if not empty\n",
    "        if first_front:\n",
    "            fronts.append(first_front)\n",
    "        else:\n",
    "            # If no non-dominated solutions found, add the best one\n",
    "            # This is a fallback to prevent empty fronts\n",
    "            min_dom_count = float('inf')\n",
    "            min_dom_idx = 0\n",
    "            for i, p in enumerate(population):\n",
    "                if p['domination_count'] < min_dom_count:\n",
    "                    min_dom_count = p['domination_count']\n",
    "                    min_dom_idx = i\n",
    "            \n",
    "            if min_dom_count < float('inf'):\n",
    "                population[min_dom_idx]['rank'] = 0\n",
    "                fronts.append([min_dom_idx])\n",
    "        \n",
    "        # Find subsequent fronts\n",
    "        i = 0\n",
    "        while i < len(fronts):  # Safe check against fronts length\n",
    "            next_front = []\n",
    "            \n",
    "            for p_idx in fronts[i]:\n",
    "                p = population[p_idx]\n",
    "                \n",
    "                for q_idx in p['dominated_solutions']:\n",
    "                    q = population[q_idx]\n",
    "                    q['domination_count'] -= 1\n",
    "                    \n",
    "                    if q['domination_count'] == 0:\n",
    "                        q['rank'] = i + 1\n",
    "                        next_front.append(q_idx)\n",
    "            \n",
    "            i += 1\n",
    "            if next_front:\n",
    "                fronts.append(next_front)\n",
    "        \n",
    "        return fronts\n",
    "    \n",
    "    def calculate_crowding_distance(self, population, front):\n",
    "        if len(front) <= 2:\n",
    "            for idx in front:\n",
    "                population[idx]['crowding_distance'] = float('inf')\n",
    "            return\n",
    "        \n",
    "        n_objectives = len(population[0]['objectives'])\n",
    "        \n",
    "        for idx in front:\n",
    "            population[idx]['crowding_distance'] = 0\n",
    "        \n",
    "        for obj_idx in range(n_objectives):\n",
    "            # Sort the front by the objective value\n",
    "            front_sorted = sorted(front, key=lambda x: population[x]['objectives'][obj_idx])\n",
    "            \n",
    "            # Set boundary points to infinity\n",
    "            population[front_sorted[0]]['crowding_distance'] = float('inf')\n",
    "            population[front_sorted[-1]]['crowding_distance'] = float('inf')\n",
    "            \n",
    "            # Calculate crowding distance for other points\n",
    "            obj_range = (population[front_sorted[-1]]['objectives'][obj_idx] - \n",
    "                         population[front_sorted[0]]['objectives'][obj_idx])\n",
    "            \n",
    "            if obj_range == 0:\n",
    "                continue  # Skip if all values are the same\n",
    "                \n",
    "            for i in range(1, len(front_sorted) - 1):\n",
    "                distance = (population[front_sorted[i+1]]['objectives'][obj_idx] - \n",
    "                           population[front_sorted[i-1]]['objectives'][obj_idx]) / obj_range\n",
    "                \n",
    "                population[front_sorted[i]]['crowding_distance'] += distance\n",
    "    \n",
    "    def tournament_selection(self, population, k=2):\n",
    "        # Ensure k is not larger than population size\n",
    "        k = min(k, len(population))\n",
    "        \n",
    "        # Select k random individuals\n",
    "        candidates = random.sample(population, k)\n",
    "        \n",
    "        # Find the best by comparing rank first, then crowding distance\n",
    "        best = candidates[0]\n",
    "        \n",
    "        for candidate in candidates[1:]:\n",
    "            # Handle case where rank might be None\n",
    "            if candidate['rank'] is None and best['rank'] is None:\n",
    "                # If both ranks are None, use crowding distance\n",
    "                if candidate['crowding_distance'] > best['crowding_distance']:\n",
    "                    best = candidate\n",
    "            elif candidate['rank'] is None:\n",
    "                continue  # Keep best if candidate rank is None\n",
    "            elif best['rank'] is None:\n",
    "                best = candidate  # Replace if best rank is None\n",
    "            # Normal comparison\n",
    "            elif candidate['rank'] < best['rank']:\n",
    "                best = candidate\n",
    "            elif candidate['rank'] == best['rank'] and candidate['crowding_distance'] > best['crowding_distance']:\n",
    "                best = candidate\n",
    "        \n",
    "        return best\n",
    "    \n",
    "    def simulated_binary_crossover(self, parent1, parent2, eta=1):\n",
    "        p1 = parent1['weights'].copy()\n",
    "        p2 = parent2['weights'].copy()\n",
    "        \n",
    "        # Apply SBX\n",
    "        if random.random() <= self.crossover_prob:\n",
    "            child1 = np.zeros_like(p1)\n",
    "            child2 = np.zeros_like(p2)\n",
    "            \n",
    "            for i in range(len(p1)):\n",
    "                if random.random() <= 0.5:\n",
    "                    if abs(p1[i] - p2[i]) > 1e-10:\n",
    "                        if p1[i] < p2[i]:\n",
    "                            y1, y2 = p1[i], p2[i]\n",
    "                        else:\n",
    "                            y1, y2 = p2[i], p1[i]\n",
    "                        \n",
    "                        beta = 1.0 + (2.0 * (y1 - 0.0)) / (y2 - y1)\n",
    "                        alpha = 2.0 - beta ** (-(eta + 1.0))\n",
    "                        \n",
    "                        rand = random.random()\n",
    "                        if rand <= 1.0 / alpha:\n",
    "                            beta_q = (rand * alpha) ** (1.0 / (eta + 1.0))\n",
    "                        else:\n",
    "                            beta_q = (1.0 / (2.0 - rand * alpha)) ** (1.0 / (eta + 1.0))\n",
    "                        \n",
    "                        c1 = 0.5 * ((y1 + y2) - beta_q * (y2 - y1))\n",
    "                        c2 = 0.5 * ((y1 + y2) + beta_q * (y2 - y1))\n",
    "                        \n",
    "                        c1 = max(0.0, min(1.0, c1))\n",
    "                        c2 = max(0.0, min(1.0, c2))\n",
    "                        \n",
    "                        child1[i] = c1\n",
    "                        child2[i] = c2\n",
    "                    else:\n",
    "                        child1[i] = p1[i]\n",
    "                        child2[i] = p2[i]\n",
    "                else:\n",
    "                    child1[i] = p1[i]\n",
    "                    child2[i] = p2[i]\n",
    "        else:\n",
    "            child1 = p1.copy()\n",
    "            child2 = p2.copy()\n",
    "        \n",
    "        # Normalize to sum to 1\n",
    "        child1 /= np.sum(child1)\n",
    "        child2 /= np.sum(child2)\n",
    "        \n",
    "        return child1, child2\n",
    "    \n",
    "    def polynomial_mutation(self, individual, eta=20, mutation_prob=None):\n",
    "        \"\"\"Apply polynomial mutation with adaptive mutation rate\"\"\"\n",
    "        if mutation_prob is None:\n",
    "            mutation_prob = self.current_mutation_rate if self.adaptive_mutation else self.mutation_prob\n",
    "            \n",
    "        mutated = individual.copy()\n",
    "        \n",
    "        for i in range(len(mutated)):\n",
    "            if random.random() <= mutation_prob:\n",
    "                # Get the boundaries\n",
    "                lb, ub = 0.0, 1.0\n",
    "                \n",
    "                # Apply polynomial mutation\n",
    "                delta1 = (mutated[i] - lb) / (ub - lb)\n",
    "                delta2 = (ub - mutated[i]) / (ub - lb)\n",
    "                \n",
    "                rand = random.random()\n",
    "                mut_pow = 1.0 / (eta + 1.0)\n",
    "                \n",
    "                if rand <= 0.5:\n",
    "                    xy = 1.0 - delta1\n",
    "                    val = 2.0 * rand + (1.0 - 2.0 * rand) * (xy ** (eta + 1.0))\n",
    "                    deltaq = val ** mut_pow - 1.0\n",
    "                else:\n",
    "                    xy = 1.0 - delta2\n",
    "                    val = 2.0 * (1.0 - rand) + 2.0 * (rand - 0.5) * (xy ** (eta + 1.0))\n",
    "                    deltaq = 1.0 - val ** mut_pow\n",
    "                \n",
    "                mutated[i] += deltaq * (ub - lb)\n",
    "                mutated[i] = max(lb, min(ub, mutated[i]))\n",
    "        \n",
    "        # Normalize to sum to 1\n",
    "        mutated /= np.sum(mutated)\n",
    "        \n",
    "        return mutated\n",
    "    \n",
    "    def create_offspring(self, population):\n",
    "        offspring = []\n",
    "        \n",
    "        while len(offspring) < self.pop_size:\n",
    "            # Tournament selection\n",
    "            parent1 = self.tournament_selection(population, self.tournament_size)\n",
    "            parent2 = self.tournament_selection(population, self.tournament_size)\n",
    "            \n",
    "            # Crossover\n",
    "            child1_weights, child2_weights = self.simulated_binary_crossover(parent1, parent2)\n",
    "            \n",
    "            # Mutation\n",
    "            child1_weights = self.polynomial_mutation(child1_weights)\n",
    "            child2_weights = self.polynomial_mutation(child2_weights)\n",
    "            \n",
    "            # Evaluate objectives\n",
    "            child1_objectives = self.evaluate_objectives(child1_weights)\n",
    "            child2_objectives = self.evaluate_objectives(child2_weights)\n",
    "            \n",
    "            # Create child individuals\n",
    "            child1 = {\n",
    "                'weights': child1_weights,\n",
    "                'objectives': child1_objectives,\n",
    "                'rank': None,\n",
    "                'crowding_distance': None\n",
    "            }\n",
    "            \n",
    "            child2 = {\n",
    "                'weights': child2_weights,\n",
    "                'objectives': child2_objectives,\n",
    "                'rank': None,\n",
    "                'crowding_distance': None\n",
    "            }\n",
    "            \n",
    "            offspring.append(child1)\n",
    "            offspring.append(child2)\n",
    "        \n",
    "        # Take only pop_size offspring\n",
    "        return offspring[:self.pop_size]\n",
    "    \n",
    "    def select_next_generation(self, population, offspring):\n",
    "        # Combine parent and offspring populations\n",
    "        combined = population + offspring\n",
    "        \n",
    "        # Apply non-dominated sorting\n",
    "        fronts = self.fast_non_dominated_sort(combined)\n",
    "        \n",
    "        # Select individuals for the next generation\n",
    "        next_gen = []\n",
    "        front_idx = 0\n",
    "        \n",
    "        while front_idx < len(fronts) and len(next_gen) + len(fronts[front_idx]) <= self.pop_size:\n",
    "            # Calculate crowding distance for the current front\n",
    "            self.calculate_crowding_distance(combined, fronts[front_idx])\n",
    "            \n",
    "            # Add all individuals from the current front\n",
    "            for idx in fronts[front_idx]:\n",
    "                next_gen.append(combined[idx])\n",
    "            \n",
    "            front_idx += 1\n",
    "        \n",
    "        # If we need more individuals, add them based on crowding distance\n",
    "        if len(next_gen) < self.pop_size and front_idx < len(fronts):\n",
    "            # Calculate crowding distance for the last front\n",
    "            self.calculate_crowding_distance(combined, fronts[front_idx])\n",
    "            \n",
    "            # Sort the last front by crowding distance\n",
    "            last_front = sorted(fronts[front_idx], \n",
    "                                key=lambda x: combined[x]['crowding_distance'],\n",
    "                                reverse=True)\n",
    "            \n",
    "            # Add individuals from the last front until the population is filled\n",
    "            remaining = self.pop_size - len(next_gen)\n",
    "            for i in range(min(remaining, len(last_front))):\n",
    "                next_gen.append(combined[last_front[i]])\n",
    "        \n",
    "        return next_gen\n",
    "    \n",
    "    def calculate_hypervolume(self, population):\n",
    "        \"\"\"Calculate hypervolume indicator for the given population.\"\"\"\n",
    "        # Find the first front (non-dominated solutions)\n",
    "        fronts = self.fast_non_dominated_sort(population)\n",
    "        \n",
    "        # Handle case where fronts might be empty\n",
    "        if not fronts:\n",
    "            return 0.0\n",
    "            \n",
    "        first_front_indices = fronts[0]\n",
    "        if not first_front_indices:\n",
    "            return 0.0\n",
    "            \n",
    "        first_front = [population[idx] for idx in first_front_indices]\n",
    "        \n",
    "        # Extract objectives of the first front\n",
    "        objectives = np.array([ind['objectives'] for ind in first_front])\n",
    "        \n",
    "        # Initialize reference point if not already set\n",
    "        if self.reference_point is None:\n",
    "            # For portfolio problem, set reasonable reference points:\n",
    "            # - For return (obj[0]): Worst return (most negative value) * 1.1\n",
    "            # - For risk (obj[1]): Highest risk value * 1.1\n",
    "            # - For diversification (obj[2] if exists): Least diversification * 1.1\n",
    "            \n",
    "            # Get worst values from all individuals\n",
    "            all_objectives = np.array([ind['objectives'] for ind in population])\n",
    "            # Objective 0 is negative return (minimizing), so take max\n",
    "            # Objective 1 is risk (minimizing), so take max\n",
    "            # Objective 2 is negative diversification (minimizing), so take max\n",
    "            worst_values = np.max(all_objectives, axis=0) * 1.1\n",
    "            \n",
    "            # Ensure the reference point is sufficiently worse than all solutions\n",
    "            self.reference_point = worst_values\n",
    "            print(f\"Set reference point: {self.reference_point}\")\n",
    "        \n",
    "        # For 2 objectives, use the sweep line algorithm\n",
    "        if self.n_objectives == 2:\n",
    "            return self._hypervolume_2d(objectives)\n",
    "        \n",
    "        # For 3 objectives, use Monte Carlo approximation\n",
    "        return self._hypervolume_3d_monte_carlo(objectives, n_samples=10000)\n",
    "    \n",
    "    def _hypervolume_2d(self, objectives):\n",
    "        \"\"\"\n",
    "        Calculate 2D hypervolume using a correct sweep line algorithm\n",
    "        \"\"\"\n",
    "        if len(objectives) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        # For minimization problems, we need to:\n",
    "        # 1. Sort by first objective (expected return, which is negated for minimization)\n",
    "        # 2. Keep track of the best second objective value seen so far\n",
    "        \n",
    "        # Sort points by first objective (ascending)\n",
    "        sorted_idx = np.argsort(objectives[:, 0])\n",
    "        sorted_objectives = objectives[sorted_idx]\n",
    "        \n",
    "        # Initialize hypervolume\n",
    "        hypervolume = 0.0\n",
    "        \n",
    "        # Keep track of the best second objective value seen so far\n",
    "        # (lowest value for second objective since we're minimizing)\n",
    "        prev_min_y = self.reference_point[1]\n",
    "        \n",
    "        # Process the points from best to worst on first objective\n",
    "        for i in range(len(sorted_objectives)):\n",
    "            # Current point\n",
    "            x = sorted_objectives[i, 0]\n",
    "            y = sorted_objectives[i, 1]\n",
    "            \n",
    "            # Only consider points that improve second objective\n",
    "            if y < prev_min_y:\n",
    "                # Calculate the area contribution of this point\n",
    "                # Width is distance from current x to previous x\n",
    "                if i > 0:\n",
    "                    width = x - sorted_objectives[i-1, 0]\n",
    "                else:\n",
    "                    width = x - 0  # Assume 0 as the lower bound for the first point\n",
    "                    \n",
    "                # Height is improvement in the second objective\n",
    "                height = prev_min_y - y\n",
    "                \n",
    "                # Add area to hypervolume\n",
    "                hypervolume += width * height\n",
    "                \n",
    "                # Update best second objective\n",
    "                prev_min_y = y\n",
    "        \n",
    "        # Add the final contribution (from last point to reference point)\n",
    "        if len(sorted_objectives) > 0:\n",
    "            last_x = sorted_objectives[-1, 0]\n",
    "            last_y = prev_min_y\n",
    "            \n",
    "            width = self.reference_point[0] - last_x\n",
    "            height = self.reference_point[1] - last_y\n",
    "            hypervolume += width * height\n",
    "        \n",
    "        return hypervolume\n",
    "    \n",
    "    def _hypervolume_3d_monte_carlo(self, objectives, n_samples=10000):\n",
    "        # Generate random samples\n",
    "        samples = np.random.uniform(\n",
    "            low=np.min(objectives, axis=0),\n",
    "            high=self.reference_point,\n",
    "            size=(n_samples, self.n_objectives)\n",
    "        )\n",
    "        \n",
    "        # Count samples in the dominated hypervolume\n",
    "        count = 0\n",
    "        for sample in samples:\n",
    "            for point in objectives:\n",
    "                if np.all(sample >= point) and np.all(sample <= self.reference_point):\n",
    "                    count += 1\n",
    "                    break\n",
    "        \n",
    "        # Calculate hypervolume as a fraction of the reference volume\n",
    "        reference_volume = np.prod(self.reference_point - np.min(objectives, axis=0))\n",
    "        hypervolume = (count / n_samples) * reference_volume\n",
    "        \n",
    "        return hypervolume\n",
    "    \n",
    "    def calculate_igd(self, population, reference_front=None):\n",
    "        # If no reference front, IGD doesn't make sense\n",
    "        if reference_front is None or len(reference_front) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        # Find the first front (non-dominated solutions)\n",
    "        fronts = self.fast_non_dominated_sort(population)\n",
    "        \n",
    "        # Handle case where fronts might be empty\n",
    "        if not fronts or len(fronts[0]) == 0:\n",
    "            return float('inf')  # Worst possible IGD if no solutions\n",
    "            \n",
    "        first_front_indices = fronts[0]\n",
    "        first_front = [population[idx] for idx in first_front_indices]\n",
    "        \n",
    "        # Extract objectives of the first front\n",
    "        objectives = np.array([ind['objectives'] for ind in first_front])\n",
    "        \n",
    "        # Calculate the minimum distance from each point in the reference front to the current front\n",
    "        distances = []\n",
    "        for ref_point in reference_front:\n",
    "            min_dist = float('inf')\n",
    "            for point in objectives:\n",
    "                dist = np.linalg.norm(ref_point - point)\n",
    "                min_dist = min(min_dist, dist)\n",
    "            distances.append(min_dist)\n",
    "        \n",
    "        # IGD is the average of these minimum distances\n",
    "        igd = np.mean(distances)\n",
    "        \n",
    "        return igd\n",
    "    \n",
    "    def calculate_population_diversity(self, population):\n",
    "        \"\"\"Calculate diversity of the population based on genotype (weights)\"\"\"\n",
    "        if not population:\n",
    "            return 0.0\n",
    "            \n",
    "        # Extract weights\n",
    "        weights = np.array([ind['weights'] for ind in population])\n",
    "        \n",
    "        # Calculate average euclidean distance between all pairs of individuals\n",
    "        n = len(weights)\n",
    "        if n <= 1:\n",
    "            return 0.0\n",
    "            \n",
    "        total_distance = 0.0\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                total_distance += np.linalg.norm(weights[i] - weights[j])\n",
    "        \n",
    "        # Normalize by number of pairs\n",
    "        diversity = total_distance / (n * (n-1) / 2)\n",
    "        return diversity\n",
    "    \n",
    "    def update_mutation_rate(self, generation):\n",
    "        \"\"\"Update mutation rate based on population diversity and convergence\"\"\"\n",
    "        # Store the current mutation rate for tracking\n",
    "        self.mutation_rate_history.append(self.current_mutation_rate)\n",
    "\n",
    "        if not self.adaptive_mutation or generation < self.convergence_window:\n",
    "            return self.mutation_prob\n",
    "        \n",
    "        # Check for convergence based on hypervolume history\n",
    "        recent_hv = self.hypervolume_history[-self.convergence_window:]\n",
    "        hv_change = np.abs(recent_hv[-1] - recent_hv[0])\n",
    "        normalized_change = hv_change / (recent_hv[0] + 1e-10)  # Avoid division by zero\n",
    "        \n",
    "        # Check diversity trend\n",
    "        if len(self.diversity_history) >= self.convergence_window:\n",
    "            recent_diversity = self.diversity_history[-self.convergence_window:]\n",
    "            diversity_trend = recent_diversity[-1] / (recent_diversity[0] + 1e-10) - 1.0\n",
    "        else:\n",
    "            diversity_trend = 0.0\n",
    "        \n",
    "        # Adjust mutation rate\n",
    "        if normalized_change < self.convergence_threshold:\n",
    "            # Increase mutation rate if converging to promote exploration\n",
    "            self.current_mutation_rate = min(self.mutation_max, \n",
    "                                        self.current_mutation_rate * 1.5)\n",
    "        elif diversity_trend < -0.1:  # Diversity is dropping\n",
    "            # Increase mutation rate to maintain diversity\n",
    "            self.current_mutation_rate = min(self.mutation_max, \n",
    "                                        self.current_mutation_rate * 1.2)\n",
    "        else:\n",
    "            # Gradually decrease mutation rate if progressing well\n",
    "            self.current_mutation_rate = max(self.mutation_min, \n",
    "                                        self.current_mutation_rate * 0.95)\n",
    "        \n",
    "        return self.current_mutation_rate\n",
    "    \n",
    "    def optimize_qp(self, target_return=None, target_risk=None, max_return=False):\n",
    "        \"\"\"\n",
    "        Use quadratic programming to find efficient portfolios\n",
    "        Either specify a target return, target risk, or set max_return=True\n",
    "        \"\"\"\n",
    "        n = self.n_assets\n",
    "        \n",
    "        if max_return:\n",
    "            # Find maximum return portfolio\n",
    "            c = matrix(-self.expected_returns)\n",
    "            \n",
    "            # Constraint: w ≥ 0\n",
    "            G = matrix(-np.eye(n))\n",
    "            h = matrix(np.zeros(n))\n",
    "            \n",
    "            # Constraint: sum(w) = 1\n",
    "            A = matrix(np.ones((1, n)))\n",
    "            b = matrix(np.ones(1))\n",
    "            \n",
    "            sol = solvers.lp(c, G, h, A, b)\n",
    "            if sol['status'] != 'optimal':\n",
    "                return None\n",
    "                \n",
    "            weights = np.array(sol['x']).flatten()\n",
    "            port_return = np.dot(weights, self.expected_returns)\n",
    "            port_risk = np.sqrt(np.dot(weights.T, np.dot(self.cov_matrix, weights)))\n",
    "            \n",
    "            return weights, -port_return, port_risk\n",
    "        \n",
    "        elif target_return is not None:\n",
    "            # Find minimum risk portfolio with target return constraint\n",
    "            P = matrix(self.cov_matrix)\n",
    "            q = matrix(np.zeros(n))\n",
    "            \n",
    "            # Constraint: w ≥ 0 and expected return >= target_return\n",
    "            G = matrix(np.vstack((-np.eye(n), -self.expected_returns.reshape(1, -1))))\n",
    "            h = matrix(np.vstack((np.zeros((n, 1)), -target_return * np.ones((1, 1)))))\n",
    "            \n",
    "            # Constraint: sum(w) = 1\n",
    "            A = matrix(np.ones((1, n)))\n",
    "            b = matrix(np.ones(1))\n",
    "            \n",
    "            try:\n",
    "                sol = solvers.qp(P, q, G, h, A, b)\n",
    "                if sol['status'] != 'optimal':\n",
    "                    return None\n",
    "                    \n",
    "                weights = np.array(sol['x']).flatten()\n",
    "                port_return = np.dot(weights, self.expected_returns)\n",
    "                port_risk = np.sqrt(np.dot(weights.T, np.dot(self.cov_matrix, weights)))\n",
    "                \n",
    "                return weights, -port_return, port_risk\n",
    "            except:\n",
    "                return None\n",
    "        \n",
    "        # Default to minimum risk portfolio\n",
    "        P = matrix(self.cov_matrix)\n",
    "        q = matrix(np.zeros(n))\n",
    "        \n",
    "        # Constraint: w ≥ 0\n",
    "        G = matrix(-np.eye(n))\n",
    "        h = matrix(np.zeros(n))\n",
    "        \n",
    "        # Constraint: sum(w) = 1\n",
    "        A = matrix(np.ones((1, n)))\n",
    "        b = matrix(np.ones(1))\n",
    "        \n",
    "        sol = solvers.qp(P, q, G, h, A, b)\n",
    "        if sol['status'] != 'optimal':\n",
    "            return None\n",
    "            \n",
    "        weights = np.array(sol['x']).flatten()\n",
    "        port_return = np.dot(weights, self.expected_returns)\n",
    "        port_risk = np.sqrt(np.dot(weights.T, np.dot(self.cov_matrix, weights)))\n",
    "        \n",
    "        return weights, -port_return, port_risk\n",
    "\n",
    "    def apply_qp_refinement(self, population):\n",
    "        \"\"\"\n",
    "        Apply QP refinement to the first front of the population\n",
    "        \"\"\"\n",
    "        # Find the first front\n",
    "        fronts = self.fast_non_dominated_sort(population)\n",
    "        if not fronts:\n",
    "            return population\n",
    "            \n",
    "        first_front_indices = fronts[0]\n",
    "        if not first_front_indices:\n",
    "            return population\n",
    "            \n",
    "        first_front = [population[idx] for idx in first_front_indices]\n",
    "        \n",
    "        # Select a subset of points to refine\n",
    "        n_to_refine = min(5, len(first_front))  # Limit the number to avoid excessive computation\n",
    "        points_to_refine = random.sample(first_front, n_to_refine)\n",
    "        \n",
    "        # Generate refined solutions\n",
    "        refined_solutions = []\n",
    "        for point in points_to_refine:\n",
    "            # Target the same return as this solution\n",
    "            target_return = -point['objectives'][0]  # Negate because our objectives are negative\n",
    "            \n",
    "            # Apply QP\n",
    "            result = self.optimize_qp(target_return=target_return)\n",
    "            \n",
    "            if result is not None:\n",
    "                weights, obj_return, obj_risk = result\n",
    "                \n",
    "                # Create individual\n",
    "                if self.n_objectives == 2:\n",
    "                    objectives = np.array([obj_return, obj_risk])\n",
    "                else:\n",
    "                    # For 3 objectives, calculate diversification\n",
    "                    diversification = -np.sum(weights > 0.01)  # Negative because we want to maximize\n",
    "                    objectives = np.array([obj_return, obj_risk, diversification])\n",
    "                \n",
    "                individual = {\n",
    "                    'weights': weights,\n",
    "                    'objectives': objectives,\n",
    "                    'rank': None,\n",
    "                    'crowding_distance': None\n",
    "                }\n",
    "                \n",
    "                refined_solutions.append(individual)\n",
    "        \n",
    "        # Add refined solutions to the population\n",
    "        refined_population = population + refined_solutions\n",
    "        \n",
    "        # Re-rank the combined population\n",
    "        fronts = self.fast_non_dominated_sort(refined_population)\n",
    "        for front in fronts:\n",
    "            self.calculate_crowding_distance(refined_population, front)\n",
    "        \n",
    "        # If population is now too large, trim it back to size\n",
    "        if len(refined_population) > self.pop_size:\n",
    "            next_gen = []\n",
    "            front_idx = 0\n",
    "            \n",
    "            while front_idx < len(fronts) and len(next_gen) + len(fronts[front_idx]) <= self.pop_size:\n",
    "                next_gen.extend([refined_population[idx] for idx in fronts[front_idx]])\n",
    "                front_idx += 1\n",
    "                \n",
    "            # If we need more individuals, add them based on crowding distance\n",
    "            if len(next_gen) < self.pop_size and front_idx < len(fronts):\n",
    "                # Sort the last front by crowding distance\n",
    "                last_front = sorted(fronts[front_idx], \n",
    "                                  key=lambda x: refined_population[x]['crowding_distance'],\n",
    "                                  reverse=True)\n",
    "                \n",
    "                # Add individuals from the last front until the population is filled\n",
    "                remaining = self.pop_size - len(next_gen)\n",
    "                for i in range(min(remaining, len(last_front))):\n",
    "                    next_gen.append(refined_population[last_front[i]])\n",
    "            \n",
    "            return next_gen\n",
    "        \n",
    "        return refined_population\n",
    "\n",
    "    def run(self, track_metrics=True, ecm_solutions=None):\n",
    "        # Initialize population\n",
    "        population = self.initialize_population()\n",
    "\n",
    "        # Record initial mutation rate\n",
    "        if self.adaptive_mutation:\n",
    "            self.mutation_rate_history.append(self.current_mutation_rate)\n",
    "        else:\n",
    "            self.mutation_rate_history.append(self.mutation_prob)\n",
    "        \n",
    "        # Reference front for IGD calculation\n",
    "        reference_front = None\n",
    "        if ecm_solutions and len(ecm_solutions) > 0:\n",
    "            try:\n",
    "                # Make sure we get weights from ECM solutions properly\n",
    "                reference_front = []\n",
    "                for sol in ecm_solutions:\n",
    "                    if len(sol) >= 1 and isinstance(sol[0], np.ndarray):\n",
    "                        weights = sol[0]\n",
    "                        reference_front.append(self.evaluate_objectives(weights))\n",
    "                \n",
    "                # Convert to array only if we have points\n",
    "                if reference_front:\n",
    "                    reference_front = np.array(reference_front)\n",
    "                else:\n",
    "                    reference_front = None\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not extract reference front from ECM solutions: {e}\")\n",
    "                reference_front = None\n",
    "        \n",
    "        # Perform non-dominated sorting and crowding distance calculation\n",
    "        fronts = self.fast_non_dominated_sort(population)\n",
    "        for front in fronts:\n",
    "            self.calculate_crowding_distance(population, front)\n",
    "        \n",
    "        # Calculate initial diversity\n",
    "        diversity = self.calculate_population_diversity(population)\n",
    "        self.diversity_history.append(diversity)\n",
    "        \n",
    "        # Track metrics at generation 0\n",
    "        if track_metrics:\n",
    "            hv = self.calculate_hypervolume(population)\n",
    "            igd = self.calculate_igd(population, reference_front)\n",
    "            self.hypervolume_history.append(hv)\n",
    "            self.igd_history.append(igd)\n",
    "        \n",
    "        # Main loop\n",
    "        for gen in range(self.max_gen):\n",
    "            # Update mutation rate if using adaptive mutation\n",
    "            if self.adaptive_mutation:\n",
    "                self.current_mutation_rate = self.update_mutation_rate(gen)\n",
    "            else:\n",
    "                self.mutation_rate_history.append(self.mutation_prob)\n",
    "            \n",
    "            # Create offspring\n",
    "            offspring = self.create_offspring(population)\n",
    "            \n",
    "            # Select next generation\n",
    "            population = self.select_next_generation(population, offspring)\n",
    "            \n",
    "            # Apply QP refinement if enabled and at the right frequency\n",
    "            if self.hybrid_optimization and (gen + 1) % self.hybrid_frequency == 0:\n",
    "                print(f\"Applying QP refinement at generation {gen + 1}\")\n",
    "                population = self.apply_qp_refinement(population)\n",
    "            \n",
    "            # Calculate diversity\n",
    "            diversity = self.calculate_population_diversity(population)\n",
    "            self.diversity_history.append(diversity)\n",
    "            \n",
    "            # Track metrics\n",
    "            if track_metrics:\n",
    "                hv = self.calculate_hypervolume(population)\n",
    "                igd = self.calculate_igd(population, reference_front)\n",
    "                self.hypervolume_history.append(hv)\n",
    "                self.igd_history.append(igd)\n",
    "            \n",
    "            # Print progress every 10 generations\n",
    "            if (gen + 1) % 10 == 0:\n",
    "                print(f\"Generation {gen + 1}/{self.max_gen} completed\")\n",
    "                if self.adaptive_mutation:\n",
    "                    print(f\"Current mutation rate: {self.current_mutation_rate:.4f}\")\n",
    "        \n",
    "        return population\n",
    "    \n",
    "    def get_pareto_front(self, population):\n",
    "        # Find the first front\n",
    "        fronts = self.fast_non_dominated_sort(population)\n",
    "        \n",
    "        if not fronts:\n",
    "            return []\n",
    "            \n",
    "        first_front_indices = fronts[0]\n",
    "        \n",
    "        # Extract individuals in the first front\n",
    "        pareto_front = [population[idx] for idx in first_front_indices]\n",
    "        \n",
    "        return pareto_front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48de2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comparison_experiment(n_assets, expected_returns, cov_matrix, asset_names, \n",
    "                           ecm_solutions=None, wsm_solutions=None,\n",
    "                           n_objectives=2, pop_size=100, max_gen=80):\n",
    "    \"\"\"\n",
    "    Run all three variants of the algorithm and compare their performance\n",
    "    \"\"\"\n",
    "    # Create proper reference front for IGD calculations\n",
    "    reference_front = None\n",
    "    if ecm_solutions and len(ecm_solutions) > 0:\n",
    "        # Extract ECM solutions for reference front\n",
    "        reference_objectives = []\n",
    "        nsga2_temp = NSGA2(\n",
    "            n_assets=n_assets, \n",
    "            expected_returns=expected_returns,\n",
    "            cov_matrix=cov_matrix,\n",
    "            n_objectives=n_objectives\n",
    "        )\n",
    "        \n",
    "        for sol in ecm_solutions:\n",
    "            if len(sol) >= 1 and isinstance(sol[0], np.ndarray):\n",
    "                weights = sol[0]\n",
    "                objectives = nsga2_temp.evaluate_objectives(weights)\n",
    "                reference_objectives.append(objectives)\n",
    "        \n",
    "        if reference_objectives:\n",
    "            reference_front = np.array(reference_objectives)\n",
    "            print(f\"Created reference front from {len(reference_front)} ECM solutions\")\n",
    "        else:\n",
    "            print(\"Warning: Could not create reference front from ECM solutions\")\n",
    "    \n",
    "    # 1. Original NSGA-II\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Running Original NSGA-II\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    nsga2_original = NSGA2(\n",
    "        n_assets=n_assets,\n",
    "        expected_returns=expected_returns,\n",
    "        cov_matrix=cov_matrix,\n",
    "        pop_size=pop_size,\n",
    "        max_gen=max_gen,\n",
    "        crossover_prob=0.9,\n",
    "        mutation_prob=0.1,\n",
    "        n_objectives=n_objectives,\n",
    "        adaptive_mutation=False,  # Turn off our modifications\n",
    "        hybrid_optimization=False\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    population_original = nsga2_original.run(track_metrics=True, ecm_solutions=ecm_solutions)\n",
    "    original_time = time.time() - start_time\n",
    "    \n",
    "    pareto_front_original = nsga2_original.get_pareto_front(population_original)\n",
    "    hv_original = nsga2_original.hypervolume_history[-1] if nsga2_original.hypervolume_history else 0\n",
    "    igd_original = nsga2_original.igd_history[-1] if nsga2_original.igd_history else float('inf')\n",
    "    \n",
    "    # 2. NSGA-II with Adaptive Mutation\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Running NSGA-II with Adaptive Mutation\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    nsga2_adaptive = NSGA2(\n",
    "        n_assets=n_assets,\n",
    "        expected_returns=expected_returns,\n",
    "        cov_matrix=cov_matrix,\n",
    "        pop_size=pop_size,\n",
    "        max_gen=max_gen,\n",
    "        crossover_prob=0.9,\n",
    "        mutation_prob=0.1,\n",
    "        n_objectives=n_objectives,\n",
    "        adaptive_mutation=True,\n",
    "        hybrid_optimization=False\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    population_adaptive = nsga2_adaptive.run(track_metrics=True, ecm_solutions=ecm_solutions)\n",
    "    adaptive_time = time.time() - start_time\n",
    "    \n",
    "    pareto_front_adaptive = nsga2_adaptive.get_pareto_front(population_adaptive)\n",
    "    hv_adaptive = nsga2_adaptive.hypervolume_history[-1] if nsga2_adaptive.hypervolume_history else 0\n",
    "    igd_adaptive = nsga2_adaptive.igd_history[-1] if nsga2_adaptive.igd_history else float('inf')\n",
    "    \n",
    "    # 3. Hybrid NSGA-II with QP\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Running Hybrid NSGA-II with QP Refinement\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    nsga2_hybrid = NSGA2(\n",
    "        n_assets=n_assets,\n",
    "        expected_returns=expected_returns,\n",
    "        cov_matrix=cov_matrix,\n",
    "        pop_size=pop_size,\n",
    "        max_gen=max_gen,\n",
    "        crossover_prob=0.9,\n",
    "        mutation_prob=0.1,\n",
    "        n_objectives=n_objectives,\n",
    "        adaptive_mutation=False,\n",
    "        hybrid_optimization=True,\n",
    "        hybrid_frequency=20  # Apply QP every 20 generations\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    population_hybrid = nsga2_hybrid.run(track_metrics=True, ecm_solutions=ecm_solutions)\n",
    "    hybrid_time = time.time() - start_time\n",
    "    \n",
    "    pareto_front_hybrid = nsga2_hybrid.get_pareto_front(population_hybrid)\n",
    "    hv_hybrid = nsga2_hybrid.hypervolume_history[-1] if nsga2_hybrid.hypervolume_history else 0\n",
    "    igd_hybrid = nsga2_hybrid.igd_history[-1] if nsga2_hybrid.igd_history else float('inf')\n",
    "    \n",
    "    # Print summary including IGD\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SUMMARY OF RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Algorithm':<30} {'Solutions':<10} {'Hypervolume':<15} {'IGD':<15} {'Time (s)':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Original NSGA-II':<30} {len(pareto_front_original):<10} {hv_original:<15.6f} {igd_original:<15.6f} {original_time:<10.2f}\")\n",
    "    print(f\"{'NSGA-II with Adaptive Mutation':<30} {len(pareto_front_adaptive):<10} {hv_adaptive:<15.6f} {igd_adaptive:<15.6f} {adaptive_time:<10.2f}\")\n",
    "    print(f\"{'Hybrid NSGA-II with QP':<30} {len(pareto_front_hybrid):<10} {hv_hybrid:<15.6f} {igd_hybrid:<15.6f} {hybrid_time:<10.2f}\")\n",
    "    \n",
    "    # Plot convergence comparison with both HV and IGD\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Plot hypervolume convergence\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(nsga2_original.hypervolume_history, label='Original NSGA-II')\n",
    "    plt.plot(nsga2_adaptive.hypervolume_history, label='Adaptive Mutation')\n",
    "    plt.plot(nsga2_hybrid.hypervolume_history, label='Hybrid with QP')\n",
    "    plt.title('Hypervolume Convergence Comparison (Higher is Better)')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Hypervolume')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot IGD convergence\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(nsga2_original.igd_history, label='Original NSGA-II')\n",
    "    plt.plot(nsga2_adaptive.igd_history, label='Adaptive Mutation')\n",
    "    plt.plot(nsga2_hybrid.igd_history, label='Hybrid with QP')\n",
    "    plt.title('IGD Convergence Comparison (Lower is Better)')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Inverted Generational Distance')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot Pareto fronts\n",
    "    if n_objectives == 2:\n",
    "        # Create a new figure for individual and combined Pareto fronts\n",
    "        plt.figure(figsize=(18, 12))\n",
    "        \n",
    "        # Individual plots for each algorithm\n",
    "        plt.subplot(2, 2, 1)\n",
    "        if pareto_front_original:\n",
    "            obj_original = np.array([ind['objectives'] for ind in pareto_front_original])\n",
    "            plt.scatter(obj_original[:, 1], -obj_original[:, 0], label='Original NSGA-II', \n",
    "                        c='blue', s=50, alpha=0.7)\n",
    "            # Add ECM and WSM for reference\n",
    "            if ecm_solutions:\n",
    "                ecm_risks = [sol[2] for sol in ecm_solutions]\n",
    "                ecm_returns = [sol[1] for sol in ecm_solutions]\n",
    "                plt.scatter(ecm_risks, ecm_returns, c='black', marker='x', label='ECM', s=30, alpha=0.4)\n",
    "            plt.title('Original NSGA-II Pareto Front')\n",
    "            plt.xlabel('Risk')\n",
    "            plt.ylabel('Expected Return')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        if pareto_front_adaptive:\n",
    "            obj_adaptive = np.array([ind['objectives'] for ind in pareto_front_adaptive])\n",
    "            plt.scatter(obj_adaptive[:, 1], -obj_adaptive[:, 0], label='Adaptive Mutation', \n",
    "                        c='orange', s=50, alpha=0.7)\n",
    "            # Add ECM and WSM for reference\n",
    "            if ecm_solutions:\n",
    "                ecm_risks = [sol[2] for sol in ecm_solutions]\n",
    "                ecm_returns = [sol[1] for sol in ecm_solutions]\n",
    "                plt.scatter(ecm_risks, ecm_returns, c='black', marker='x', label='ECM', s=30, alpha=0.4)\n",
    "            plt.title('Adaptive Mutation Pareto Front')\n",
    "            plt.xlabel('Risk')\n",
    "            plt.ylabel('Expected Return')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "        \n",
    "        plt.subplot(2, 2, 3)\n",
    "        if pareto_front_hybrid:\n",
    "            obj_hybrid = np.array([ind['objectives'] for ind in pareto_front_hybrid])\n",
    "            plt.scatter(obj_hybrid[:, 1], -obj_hybrid[:, 0], label='Hybrid with QP', \n",
    "                        c='green', s=50, alpha=0.7)\n",
    "            # Add ECM and WSM for reference\n",
    "            if ecm_solutions:\n",
    "                ecm_risks = [sol[2] for sol in ecm_solutions]\n",
    "                ecm_returns = [sol[1] for sol in ecm_solutions]\n",
    "                plt.scatter(ecm_risks, ecm_returns, c='black', marker='x', label='ECM', s=30, alpha=0.4)\n",
    "            plt.title('Hybrid with QP Pareto Front')\n",
    "            plt.xlabel('Risk')\n",
    "            plt.ylabel('Expected Return')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "        \n",
    "        # Combined plot with clearer distinction\n",
    "        plt.subplot(2, 2, 4)\n",
    "        \n",
    "        # Plot with different marker styles and sizes\n",
    "        if pareto_front_original:\n",
    "            plt.scatter(obj_original[:, 1], -obj_original[:, 0], label='Original NSGA-II', \n",
    "                        c='blue', marker='o', s=40, alpha=0.6)\n",
    "        \n",
    "        if pareto_front_adaptive:\n",
    "            plt.scatter(obj_adaptive[:, 1], -obj_adaptive[:, 0], label='Adaptive Mutation', \n",
    "                        c='orange', marker='^', s=50, alpha=0.6)\n",
    "        \n",
    "        if pareto_front_hybrid:\n",
    "            plt.scatter(obj_hybrid[:, 1], -obj_hybrid[:, 0], label='Hybrid with QP', \n",
    "                        c='green', marker='s', s=60, alpha=0.6)\n",
    "        \n",
    "        # Add ECM and WSM solutions if available\n",
    "        if ecm_solutions:\n",
    "            ecm_risks = [sol[2] for sol in ecm_solutions]\n",
    "            ecm_returns = [sol[1] for sol in ecm_solutions]\n",
    "            plt.scatter(ecm_risks, ecm_returns, c='black', marker='x', label='ECM', s=30)\n",
    "        \n",
    "        if wsm_solutions:\n",
    "            wsm_risks = [sol[2] for sol in wsm_solutions]\n",
    "            wsm_returns = [sol[1] for sol in wsm_solutions]\n",
    "            plt.scatter(wsm_risks, wsm_returns, c='gray', marker='+', label='WSM', s=30)\n",
    "        \n",
    "        plt.title('Combined Pareto Front Comparison')\n",
    "        plt.xlabel('Risk')\n",
    "        plt.ylabel('Expected Return')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Create an additional comparison plot highlighting dominance\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Create convex hulls to better visualize the Pareto front extent\n",
    "        from scipy.spatial import ConvexHull\n",
    "        \n",
    "        if pareto_front_original:\n",
    "            obj_original_points = np.column_stack((obj_original[:, 1], -obj_original[:, 0]))\n",
    "            if len(obj_original_points) > 2:  # Need at least 3 points for convex hull\n",
    "                try:\n",
    "                    hull = ConvexHull(obj_original_points)\n",
    "                    hull_indices = hull.vertices\n",
    "                    plt.fill(obj_original_points[hull_indices, 0], obj_original_points[hull_indices, 1], \n",
    "                             'blue', alpha=0.2, label='Original NSGA-II Area')\n",
    "                except:\n",
    "                    # If convex hull fails, just plot the points\n",
    "                    plt.scatter(obj_original[:, 1], -obj_original[:, 0], c='blue', marker='o', \n",
    "                             s=40, alpha=0.6, label='Original NSGA-II')\n",
    "            else:\n",
    "                plt.scatter(obj_original[:, 1], -obj_original[:, 0], c='blue', marker='o', \n",
    "                         s=40, alpha=0.6, label='Original NSGA-II')\n",
    "        \n",
    "        if pareto_front_adaptive:\n",
    "            obj_adaptive_points = np.column_stack((obj_adaptive[:, 1], -obj_adaptive[:, 0]))\n",
    "            if len(obj_adaptive_points) > 2:\n",
    "                try:\n",
    "                    hull = ConvexHull(obj_adaptive_points)\n",
    "                    hull_indices = hull.vertices\n",
    "                    plt.fill(obj_adaptive_points[hull_indices, 0], obj_adaptive_points[hull_indices, 1], \n",
    "                             'orange', alpha=0.2, label='Adaptive Mutation Area')\n",
    "                except:\n",
    "                    plt.scatter(obj_adaptive[:, 1], -obj_adaptive[:, 0], c='orange', marker='^', \n",
    "                             s=50, alpha=0.6, label='Adaptive Mutation')\n",
    "            else:\n",
    "                plt.scatter(obj_adaptive[:, 1], -obj_adaptive[:, 0], c='orange', marker='^', \n",
    "                         s=50, alpha=0.6, label='Adaptive Mutation')\n",
    "        \n",
    "        if pareto_front_hybrid:\n",
    "            obj_hybrid_points = np.column_stack((obj_hybrid[:, 1], -obj_hybrid[:, 0]))\n",
    "            if len(obj_hybrid_points) > 2:\n",
    "                try:\n",
    "                    hull = ConvexHull(obj_hybrid_points)\n",
    "                    hull_indices = hull.vertices\n",
    "                    plt.fill(obj_hybrid_points[hull_indices, 0], obj_hybrid_points[hull_indices, 1], \n",
    "                             'green', alpha=0.2, label='Hybrid with QP Area')\n",
    "                except:\n",
    "                    plt.scatter(obj_hybrid[:, 1], -obj_hybrid[:, 0], c='green', marker='s', \n",
    "                             s=60, alpha=0.6, label='Hybrid with QP')\n",
    "            else:\n",
    "                plt.scatter(obj_hybrid[:, 1], -obj_hybrid[:, 0], c='green', marker='s', \n",
    "                         s=60, alpha=0.6, label='Hybrid with QP')\n",
    "        \n",
    "        # Plot the actual points on top of the convex hulls\n",
    "        if pareto_front_original:\n",
    "            plt.scatter(obj_original[:, 1], -obj_original[:, 0], c='blue', marker='o', \n",
    "                     s=40, alpha=0.8, label='_nolegend_')\n",
    "        \n",
    "        if pareto_front_adaptive:\n",
    "            plt.scatter(obj_adaptive[:, 1], -obj_adaptive[:, 0], c='orange', marker='^', \n",
    "                     s=50, alpha=0.8, label='_nolegend_')\n",
    "        \n",
    "        if pareto_front_hybrid:\n",
    "            plt.scatter(obj_hybrid[:, 1], -obj_hybrid[:, 0], c='green', marker='s', \n",
    "                     s=60, alpha=0.8, label='_nolegend_')\n",
    "        \n",
    "        plt.title('Pareto Front Comparison (with Dominated Areas)')\n",
    "        plt.xlabel('Risk')\n",
    "        plt.ylabel('Expected Return')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "    else:  # 3D plot for 3 objectives\n",
    "        # Create a new figure for 3D plot\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        ax = plt.axes(projection='3d')\n",
    "        \n",
    "        # Get objectives for each variant\n",
    "        if pareto_front_original:\n",
    "            obj_original = np.array([ind['objectives'] for ind in pareto_front_original])\n",
    "            ax.scatter3D(-obj_original[:, 0], obj_original[:, 1], -obj_original[:, 2], \n",
    "                         label='Original NSGA-II', s=50, alpha=0.7)\n",
    "        \n",
    "        if pareto_front_adaptive:\n",
    "            obj_adaptive = np.array([ind['objectives'] for ind in pareto_front_adaptive])\n",
    "            ax.scatter3D(-obj_adaptive[:, 0], obj_adaptive[:, 1], -obj_adaptive[:, 2], \n",
    "                         label='Adaptive Mutation', s=50, alpha=0.7)\n",
    "        \n",
    "        if pareto_front_hybrid:\n",
    "            obj_hybrid = np.array([ind['objectives'] for ind in pareto_front_hybrid])\n",
    "            ax.scatter3D(-obj_hybrid[:, 0], obj_hybrid[:, 1], -obj_hybrid[:, 2], \n",
    "                         label='Hybrid with QP', s=50, alpha=0.7)\n",
    "        \n",
    "        ax.set_title('3D Pareto Front Comparison')\n",
    "        ax.set_xlabel('Expected Return')\n",
    "        ax.set_ylabel('Risk')\n",
    "        ax.set_zlabel('Diversification')\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot diversity comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(nsga2_original.diversity_history, label='Original NSGA-II')\n",
    "    plt.plot(nsga2_adaptive.diversity_history, label='Adaptive Mutation')\n",
    "    plt.plot(nsga2_hybrid.diversity_history, label='Hybrid with QP')\n",
    "    plt.title('Population Diversity Comparison')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Diversity')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate Sharpe ratios for each front\n",
    "    best_solutions = {}\n",
    "    fronts = {\n",
    "        'Original NSGA-II': pareto_front_original,\n",
    "        'Adaptive Mutation': pareto_front_adaptive,\n",
    "        'Hybrid with QP': pareto_front_hybrid\n",
    "    }\n",
    "    \n",
    "    for name, front in fronts.items():\n",
    "        if not front:\n",
    "            continue\n",
    "            \n",
    "        objectives = np.array([ind['objectives'] for ind in front])\n",
    "        returns = -objectives[:, 0]\n",
    "        risks = objectives[:, 1]\n",
    "        \n",
    "        # Find maximum Sharpe ratio\n",
    "        sharpe_ratios = returns / risks\n",
    "        max_sharpe_idx = np.argmax(sharpe_ratios)\n",
    "        best_sol = front[max_sharpe_idx]\n",
    "        \n",
    "        best_solutions[name] = {\n",
    "            'solution': best_sol,\n",
    "            'return': returns[max_sharpe_idx],\n",
    "            'risk': risks[max_sharpe_idx],\n",
    "            'sharpe': sharpe_ratios[max_sharpe_idx]\n",
    "        }\n",
    "    \n",
    "    # Print best portfolio information\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"BEST PORTFOLIO COMPARISON (Maximum Sharpe Ratio)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Algorithm':<30} {'Return':<10} {'Risk':<10} {'Sharpe Ratio':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for name, data in best_solutions.items():\n",
    "        print(f\"{name:<30} {data['return']:<10.4f} {data['risk']:<10.6f} {data['sharpe']:<15.4f}\")\n",
    "    \n",
    "    # Visualize the best portfolio weights from each algorithm\n",
    "    for name, data in best_solutions.items():\n",
    "        # Find assets with significant weights (>1%)\n",
    "        weights = data['solution']['weights']\n",
    "        significant_weights = [(asset, weight) for asset, weight in zip(asset_names, weights) if weight > 0.01]\n",
    "        significant_weights.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Plot as a bar chart\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        assets = [item[0] for item in significant_weights]\n",
    "        weight_values = [item[1] * 100 for item in significant_weights]\n",
    "        \n",
    "        plt.bar(assets, weight_values)\n",
    "        plt.xlabel('Asset')\n",
    "        plt.ylabel('Weight (%)')\n",
    "        plt.title(f\"Best Portfolio Weights - {name}\\nReturn: {data['return']:.4f}, Risk: {data['risk']:.6f}, Sharpe: {data['sharpe']:.4f}\")\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Additional 3D visualization for three-objective case\n",
    "    if n_objectives == 3:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        ax = plt.axes(projection='3d')\n",
    "        \n",
    "        # Create a custom colormap to distinguish different fronts\n",
    "        colors = ['red', 'blue', 'green']\n",
    "        markers = ['o', '^', 's']\n",
    "        \n",
    "        for i, (name, front) in enumerate(fronts.items()):\n",
    "            if not front:\n",
    "                continue\n",
    "                \n",
    "            objectives = np.array([ind['objectives'] for ind in front])\n",
    "            \n",
    "            # Project colormap to objectives\n",
    "            ax.scatter3D(-objectives[:, 0], objectives[:, 1], -objectives[:, 2], \n",
    "                        c=colors[i], marker=markers[i], label=name, s=50, alpha=0.7)\n",
    "        \n",
    "        ax.set_title('3D Pareto Front Comparison with Multiple Views')\n",
    "        ax.set_xlabel('Expected Return')\n",
    "        ax.set_ylabel('Risk')\n",
    "        ax.set_zlabel('Diversification')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Show plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Generate multiple views for better visualization\n",
    "        for angle in [0, 45, 90, 135]:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            ax = plt.axes(projection='3d')\n",
    "            \n",
    "            for i, (name, front) in enumerate(fronts.items()):\n",
    "                if not front:\n",
    "                    continue\n",
    "                    \n",
    "                objectives = np.array([ind['objectives'] for ind in front])\n",
    "                ax.scatter3D(-objectives[:, 0], objectives[:, 1], -objectives[:, 2], \n",
    "                            c=colors[i], marker=markers[i], label=name, s=50, alpha=0.7)\n",
    "            \n",
    "            ax.view_init(elev=30, azim=angle)\n",
    "            ax.set_title(f'3D Pareto Front (View Angle: {angle}°)')\n",
    "            ax.set_xlabel('Expected Return')\n",
    "            ax.set_ylabel('Risk')\n",
    "            ax.set_zlabel('Diversification')\n",
    "            ax.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    return {\n",
    "        'original': {\n",
    "            'nsga2': nsga2_original,\n",
    "            'population': population_original,\n",
    "            'pareto_front': pareto_front_original,\n",
    "            'hypervolume': hv_original,\n",
    "            'execution_time': original_time\n",
    "        },\n",
    "        'adaptive': {\n",
    "            'nsga2': nsga2_adaptive,\n",
    "            'population': population_adaptive,\n",
    "            'pareto_front': pareto_front_adaptive,\n",
    "            'hypervolume': hv_adaptive,\n",
    "            'execution_time': adaptive_time\n",
    "        },\n",
    "        'hybrid': {\n",
    "            'nsga2': nsga2_hybrid,\n",
    "            'population': population_hybrid,\n",
    "            'pareto_front': pareto_front_hybrid,\n",
    "            'hypervolume': hv_hybrid,\n",
    "            'execution_time': hybrid_time\n",
    "        },\n",
    "        'best_solutions': best_solutions\n",
    "    }\n",
    "\n",
    "def plot_mutation_rates(results):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot for Original NSGA-II (should be constant)\n",
    "    original_rates = results['original']['nsga2'].mutation_rate_history\n",
    "    generations = list(range(len(original_rates)))\n",
    "    plt.plot(generations, original_rates, label='Original NSGA-II', color='blue')\n",
    "    \n",
    "    # Plot for Adaptive Mutation\n",
    "    adaptive_rates = results['adaptive']['nsga2'].mutation_rate_history\n",
    "    generations_adaptive = list(range(len(adaptive_rates)))\n",
    "    plt.plot(generations_adaptive, adaptive_rates, label='Adaptive Mutation', color='orange')\n",
    "    \n",
    "    # Plot for Hybrid QP (should be constant like original)\n",
    "    hybrid_rates = results['hybrid']['nsga2'].mutation_rate_history\n",
    "    generations_hybrid = list(range(len(hybrid_rates)))\n",
    "    plt.plot(generations_hybrid, hybrid_rates, label='Hybrid with QP', color='green')\n",
    "    \n",
    "    plt.title('Mutation Rate Changes Over Generations')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Mutation Rate')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.ylim(bottom=0)  # Start y-axis at 0\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to analyze the relationship between IGD and Hypervolume\n",
    "def analyze_metric_correlation(results):\n",
    "    \"\"\"Analyze correlation between IGD and Hypervolume metrics\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Scatter plot of IGD vs HV for all algorithms\n",
    "    plt.subplot(1, 2, 1)\n",
    "    \n",
    "    # Extract data\n",
    "    for name, data in [('Original', results['original']), \n",
    "                       ('Adaptive', results['adaptive']), \n",
    "                       ('Hybrid', results['hybrid'])]:\n",
    "        igd_history = data['nsga2'].igd_history\n",
    "        hv_history = data['nsga2'].hypervolume_history\n",
    "        \n",
    "        # Handle potentially different lengths\n",
    "        min_len = min(len(igd_history), len(hv_history))\n",
    "        if min_len > 0:\n",
    "            plt.scatter(igd_history[:min_len], hv_history[:min_len], \n",
    "                      alpha=0.6, label=f'{name}')\n",
    "    \n",
    "    plt.title('IGD vs Hypervolume')\n",
    "    plt.xlabel('IGD (Lower is Better)')\n",
    "    plt.ylabel('Hypervolume (Higher is Better)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 2: Comparison of metrics over generations\n",
    "    plt.subplot(1, 2, 2)\n",
    "    \n",
    "    # Use original NSGA-II as example\n",
    "    igd = np.array(results['original']['nsga2'].igd_history)\n",
    "    hv = np.array(results['original']['nsga2'].hypervolume_history)\n",
    "    \n",
    "    # Normalize to [0,1] for easier comparison\n",
    "    if len(igd) > 0 and len(hv) > 0:\n",
    "        igd_norm = (igd - np.min(igd)) / (np.max(igd) - np.min(igd) + 1e-10)\n",
    "        hv_norm = (hv - np.min(hv)) / (np.max(hv) - np.min(hv) + 1e-10)\n",
    "        \n",
    "        # Invert IGD so higher is better (for visualization)\n",
    "        igd_norm = 1 - igd_norm\n",
    "        \n",
    "        plt.plot(igd_norm, label='IGD (inverted, normalized)')\n",
    "        plt.plot(hv_norm, label='HV (normalized)')\n",
    "        plt.title('Normalized Metric Comparison (Higher is Better)')\n",
    "        plt.xlabel('Generation')\n",
    "        plt.ylabel('Normalized Value')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fbc73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress solver output for cleaner console output\n",
    "solvers.options['show_progress'] = False\n",
    "\n",
    "# Make sure ecm_solutions and wsm_solutions are defined (from your previous code)\n",
    "if 'ecm_solutions' not in globals() or ecm_solutions is None:\n",
    "    print(\"No ECM solutions found. Creating empty list.\")\n",
    "    ecm_solutions = []\n",
    "\n",
    "if 'wsm_solutions' not in globals() or wsm_solutions is None:\n",
    "    print(\"No WSM solutions found. Creating empty list.\")\n",
    "    wsm_solutions = []\n",
    "\n",
    "# Run comparison experiment for 2 objectives\n",
    "results = run_comparison_experiment(\n",
    "    n_assets=n_assets, \n",
    "    expected_returns=expected_returns, \n",
    "    cov_matrix=cov_matrix,\n",
    "    asset_names=asset_names,\n",
    "    ecm_solutions=ecm_solutions,\n",
    "    wsm_solutions=wsm_solutions,\n",
    "    n_objectives=2,\n",
    "    pop_size=100,\n",
    "    max_gen=100\n",
    ")\n",
    "# Now plot the mutation rates\n",
    "plot_mutation_rates(results)\n",
    "# Analyze correlation between IGD and Hypervolume\n",
    "analyze_metric_correlation(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7515458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comparison experiment for 3 objectives\n",
    "results_3d = run_comparison_experiment(\n",
    "    n_assets=n_assets, \n",
    "    expected_returns=expected_returns, \n",
    "    cov_matrix=cov_matrix,\n",
    "    asset_names=asset_names,\n",
    "    ecm_solutions=ecm_solutions,\n",
    "    wsm_solutions=wsm_solutions,\n",
    "    n_objectives=3,\n",
    "    pop_size=100,\n",
    "    max_gen=100\n",
    ")\n",
    "# Now plot the mutation rates\n",
    "plot_mutation_rates(results_3d)\n",
    "# Analyze correlation between IGD and Hypervolume\n",
    "analyze_metric_correlation(results_3d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

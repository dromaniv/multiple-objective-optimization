{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c83d287",
   "metadata": {},
   "source": [
    "# Multiple-objective portfolio optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1461c77",
   "metadata": {},
   "source": [
    "# INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed746013",
   "metadata": {},
   "source": [
    "Your task is to solve a multiple-objective portfolio optimization problem.\n",
    "-  Use the basic Markowitz's model from 1952 (see Lecture 1)\n",
    "-  Solve = construct Pareto front approximations.\n",
    "-  The dataset is the same as for the portfolio game part 1 (bundle1.zip).\n",
    "-  The dataset consists of the historical prices of 20 assets.\n",
    "-  The bundle contains 20 files (*.txt) linked to different assets.\n",
    "-  The name of the file suggests the asset's name.\n",
    "-  The structure of every file is as follows:\n",
    "1.  The first line contains the name of the asset.\n",
    "2. The second line provides the number of data points N.\n",
    "3. The following N lines are data points with the structure: time, price.\n",
    "-  The historical timeline for all assets is time $\\in$ [0,100].\n",
    "-  Future predictions should be calculated for time = 200.\n",
    "\n",
    "Goal: \n",
    "-  Load data, make predictions, and build the model. \n",
    "-  Illustrate your predictions (can be done in the jupyter notebook)\n",
    "-  Then, implement the WSM and ECM methods (see the tutorial on quadratic programming provided below). \n",
    "-  Run your implementations for different calculation limits (e.g., the number of weight vectors for WSM). Compare the methods' efficiency in finding unique Pareto optimal solutions. Finally, illustrate generated Pareto fronts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f2ed7",
   "metadata": {},
   "source": [
    "# Short tutorial on the cvxopt library for quadratic programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2ce0f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cvxopt import matrix, solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c7f50c",
   "metadata": {},
   "source": [
    "# QP Optimization Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f101f05",
   "metadata": {},
   "source": [
    "### General model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d539390b",
   "metadata": {},
   "source": [
    "$max$ $\\boldsymbol{cx} - \\dfrac{1}{2}\\boldsymbol{x}^T\\boldsymbol{Qx}$ <br>\n",
    "$s.t.$ <br>\n",
    "$\\boldsymbol{Gx} \\leq \\boldsymbol{h}$ <br>\n",
    "$\\boldsymbol{x} \\geq \\boldsymbol{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfd769b",
   "metadata": {},
   "source": [
    "### But the library uses the following form:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f98165",
   "metadata": {},
   "source": [
    "$min$ $\\boldsymbol{cx} + \\dfrac{1}{2}\\boldsymbol{x}^T\\boldsymbol{Qx}$ <br>\n",
    "$s.t.$ <br>\n",
    "$\\boldsymbol{Gx} \\leq \\boldsymbol{h}$ <br>\n",
    "$\\boldsymbol{Ax} = \\boldsymbol{b}$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebb5d77",
   "metadata": {},
   "source": [
    "### Exmple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af73bb6",
   "metadata": {},
   "source": [
    "$min$ $2x^2_1+x_2^2+x_1x_2+x_1+x_2$ <br>\n",
    "$s.t.$ <br>\n",
    "$x_1 \\geq 0$<br>\n",
    "$x_2 \\geq 0$<br>\n",
    "$x_1 + x_2 = 1$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9eca3b",
   "metadata": {},
   "source": [
    "### Hence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d312e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = matrix([ [4.0, 1.0], [1.0, 2.0] ]) ## [4, 1] is 1st column, not row!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "51bed5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = matrix([1.0, 1.0]) ### (1, 2) = dimensions (1 row and 2 columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0cdbd334",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = matrix([1.0, 1.0], (1,2)) ### (1, 2) = dimensions (1 row and 2 columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "699075a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = matrix(1.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9a5b19b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = matrix([[-1.0,0.0],[0.0,-1.0]]) ### multiplied both sides by -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e50610ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = matrix([0.0,0.0]) ### multiplied both sides by -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aa92370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "solQP=solvers.qp(Q, c, G, h, A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8e136a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['x', 'y', 's', 'z', 'status', 'gap', 'relative gap', 'primal objective', 'dual objective', 'primal infeasibility', 'dual infeasibility', 'primal slack', 'dual slack', 'iterations'])\n"
     ]
    }
   ],
   "source": [
    "print(solQP.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1954e8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.50e-01]\n",
      "[ 7.50e-01]\n",
      "\n",
      "1.8750000000000182\n"
     ]
    }
   ],
   "source": [
    "print(solQP['x'])\n",
    "print(solQP['primal objective'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dc8c5d",
   "metadata": {},
   "source": [
    "# We can also solve LP problems:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb07630b",
   "metadata": {},
   "source": [
    "$min$ $\\boldsymbol{c}\\boldsymbol{x}$ <br>\n",
    "$s.t.$ <br>\n",
    "$\\boldsymbol{Gx} \\leq \\boldsymbol{h}$ <br>\n",
    "$\\boldsymbol{Ax} = \\boldsymbol{b}$ (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdccac67",
   "metadata": {},
   "source": [
    "### Exmple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e439d1a",
   "metadata": {},
   "source": [
    "$min$ $2x_1+x_2$ <br>\n",
    "$s.t.$ <br>\n",
    "$-x_1 +x_2 \\leq 1$ <br>\n",
    "$x_1 + x_2 \\geq 2$ <br>\n",
    "$x_2 \\geq 0$<br>\n",
    "$x_1 - 2x_2 \\leq 4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cd985a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = matrix([ [-1.0, -1.0, 0.0, 1.0], [1.0, -1.0, -1.0, -2.0] ])\n",
    "h = matrix([ 1.0, -2.0, 0.0, 4.0 ])\n",
    "c = matrix([ 2.0, 1.0 ])\n",
    "solLP = solvers.lp(c,G,h)  \n",
    "###!!!! OPTIONALLY A and b can be provided (equality constraints) as in solQP=solvers.qp(Q, c, G, h, A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "15e6bb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['x', 'y', 's', 'z', 'status', 'gap', 'relative gap', 'primal objective', 'dual objective', 'primal infeasibility', 'dual infeasibility', 'primal slack', 'dual slack', 'residual as primal infeasibility certificate', 'residual as dual infeasibility certificate', 'iterations'])\n"
     ]
    }
   ],
   "source": [
    "print(solLP.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "debd4e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.00e-01]\n",
      "[ 1.50e+00]\n",
      "\n",
      "2.499999989554308\n"
     ]
    }
   ],
   "source": [
    "print(solLP['x'])\n",
    "print(solLP['primal objective'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe7410",
   "metadata": {},
   "source": [
    "# Portfolio optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644ad0b3",
   "metadata": {},
   "source": [
    "Import the necessary libraries (numpy, pandas, matplotlib, cvxopt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3363d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# CELL 1: Imports\n",
    "################################################################################\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "# For reproducible randomness (if needed)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e074506",
   "metadata": {},
   "source": [
    "Load the actual data from your 20 (or more) *Part1.txt files. Each file’s first line is the asset name, second line is N, and then N lines with “time price”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6d26dd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 assets.\n",
      "First few asset names: ['SafeAndCare', 'Moneymakers', 'Fuel4', 'CPU-XYZ', 'MarsProject']\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# CELL 2: Data Loading\n",
    "#   - We assume you have ~20 text files in a \"data\" folder, each named *Part1.txt\n",
    "#   - Structure of each file:\n",
    "#       1) Asset name (string)\n",
    "#       2) Number of data points N\n",
    "#       3) N lines of \"time price\"\n",
    "################################################################################\n",
    "\n",
    "data_folder = \"data\"\n",
    "\n",
    "# Gather all Part1.txt files in the data folder\n",
    "txt_files = [f for f in os.listdir(data_folder) if f.endswith(\"Part1.txt\")]\n",
    "\n",
    "asset_names = []\n",
    "asset_times = []\n",
    "asset_prices = []\n",
    "\n",
    "for fname in txt_files:\n",
    "    path = os.path.join(data_folder, fname)\n",
    "    with open(path, \"r\") as f:\n",
    "        # 1) asset name\n",
    "        asset_name = f.readline().strip()\n",
    "\n",
    "        # 2) number of data points\n",
    "        N_line = f.readline().strip()\n",
    "        N = int(N_line)\n",
    "\n",
    "        # 3) read time, price lines\n",
    "        times = []\n",
    "        prices = []\n",
    "        for _ in range(N):\n",
    "            line = f.readline().strip()\n",
    "            t_str, p_str = line.split()\n",
    "            times.append(float(t_str))\n",
    "            prices.append(float(p_str))\n",
    "\n",
    "        asset_names.append(asset_name)\n",
    "        asset_times.append(times)\n",
    "        asset_prices.append(prices)\n",
    "\n",
    "print(f\"Found {len(asset_names)} assets.\")\n",
    "print(\"First few asset names:\", asset_names[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aa396a",
   "metadata": {},
   "source": [
    "Perform a simple linear regression (degree=1) for each asset using times in [0,100]. Extrapolate to time=200 to get a predicted price. Convert that predicted price growth into a predicted return \\mu[i]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fdef8ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asset: SafeAndCare\n",
      "   Fitted price at t=100: 6.455\n",
      "   Predicted price at t=200: 5.575\n",
      "   => Predicted return = -13.64%\n",
      "Asset: Moneymakers\n",
      "   Fitted price at t=100: 4.580\n",
      "   Predicted price at t=200: 6.541\n",
      "   => Predicted return = 42.83%\n",
      "Asset: Fuel4\n",
      "   Fitted price at t=100: 5.643\n",
      "   Predicted price at t=200: 3.480\n",
      "   => Predicted return = -38.34%\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# CELL 3: Forecasting / Predictions\n",
    "#   - We'll do a simple linear regression for each asset using data up to time=100.\n",
    "#   - We'll extrapolate to predict the price at time=200.\n",
    "#   - Then, interpret that predicted growth from t=100 to t=200 as an *expected return*.\n",
    "#\n",
    "#   A more sophisticated approach might do:\n",
    "#       * polynomial fits\n",
    "#       * log-transformed fits\n",
    "#       * ARIMA or other time series modeling\n",
    "################################################################################\n",
    "\n",
    "predicted_prices_200 = []\n",
    "current_prices_100 = []\n",
    "\n",
    "for i in range(len(asset_names)):\n",
    "    times = np.array(asset_times[i])\n",
    "    prices = np.array(asset_prices[i])\n",
    "\n",
    "    # Fit a line: price(t) ~ a * t + b\n",
    "    # We'll filter times up to t=100. (They should all be up to 100 anyway.)\n",
    "    # But let's just ensure we only use [0..100].\n",
    "    # If data indeed stops at exactly 100, we use them all.\n",
    "    mask = (times >= 0) & (times <= 100)\n",
    "    t_used = times[mask]\n",
    "    p_used = prices[mask]\n",
    "\n",
    "    # Perform linear regression with np.polyfit (degree=1)\n",
    "    coeffs = np.polyfit(t_used, p_used, deg=1)\n",
    "    # coeffs = [a, b] => price(t) = a*t + b\n",
    "\n",
    "    # Evaluate the fitted line at t=100 and t=200\n",
    "    price_at_100 = np.polyval(coeffs, 100)\n",
    "    price_at_200 = np.polyval(coeffs, 200)\n",
    "\n",
    "    predicted_prices_200.append(price_at_200)\n",
    "    current_prices_100.append(price_at_100)\n",
    "\n",
    "# Convert to arrays\n",
    "predicted_prices_200 = np.array(predicted_prices_200)\n",
    "current_prices_100 = np.array(current_prices_100)\n",
    "\n",
    "# Example: The \"forecasted return\" from time=100 to time=200\n",
    "# We'll define a simple measure: r_i = (price200 - price100)/price100\n",
    "predicted_returns = (predicted_prices_200 - current_prices_100) / current_prices_100\n",
    "\n",
    "# Let's illustrate the first few predictions:\n",
    "for i in range(min(3, len(asset_names))):\n",
    "    print(f\"Asset: {asset_names[i]}\")\n",
    "    print(f\"   Fitted price at t=100: {current_prices_100[i]:.3f}\")\n",
    "    print(f\"   Predicted price at t=200: {predicted_prices_200[i]:.3f}\")\n",
    "    print(f\"   => Predicted return = {predicted_returns[i]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19020011",
   "metadata": {},
   "source": [
    "Illustrate your predictions by plotting the historical data and the linear fit for a few assets, plus the forecasted price at t=200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba57b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# CELL 4: Illustrate Predictions\n",
    "#   - Let's plot the linear fit vs. actual data for a couple of assets, just as proof-of-concept.\n",
    "################################################################################\n",
    "\n",
    "num_to_plot = min(3, len(asset_names))  # plot a few assets\n",
    "for i in range(num_to_plot):\n",
    "    times = np.array(asset_times[i])\n",
    "    prices = np.array(asset_prices[i])\n",
    "\n",
    "    # Fit again\n",
    "    coeffs = np.polyfit(times, prices, deg=1)\n",
    "    fit_line = np.polyval(coeffs, times)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(times, prices, label=\"Historical prices\")\n",
    "    plt.plot(times, fit_line, label=\"Linear fit\")\n",
    "\n",
    "    # We'll also show the predicted price at t=200\n",
    "    price_200 = np.polyval(coeffs, 200)\n",
    "    plt.scatter([200], [price_200], marker='x', label=\"Forecast @ t=200\")\n",
    "\n",
    "    plt.title(f\"Price vs Time for {asset_names[i]}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0371b04",
   "metadata": {},
   "source": [
    "Build the Markowitz model inputs.\n",
    "\t•\t\\mu[i] comes from the predicted returns (time=100 to time=200).\n",
    "\t•\t\\Sigma is estimated from the historical returns from the adjacency in your original price data (0..100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cb42fa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_historical_returns shape: (100, 20)\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# CELL 5: Build Markowitz Model Inputs\n",
    "#   - We have a predicted return for each asset from t=100 -> t=200: predicted_returns\n",
    "#       => We can treat this as our \"mu[i]\" (expected return).\n",
    "#   - Next, we need a covariance matrix Sigma from historical returns (0..100).\n",
    "#   - We'll do standard sample covariance from discrete returns from t_{k} to t_{k+1}.\n",
    "################################################################################\n",
    "\n",
    "# Let's compute historical returns from the raw price data for times in [0..100].\n",
    "# We'll treat each adjacent time step as a \"return.\"\n",
    "# We assume each asset has the same # of points from t=0..100 for simplicity.\n",
    "\n",
    "all_historical_returns = []  # shape: (T, n_assets), T ~ 100 for example\n",
    "\n",
    "n_assets = len(asset_names)\n",
    "max_periods = None\n",
    "\n",
    "for i in range(n_assets):\n",
    "    p = np.array(asset_prices[i])\n",
    "    # We'll do the discrete return for consecutive data points:\n",
    "    # r_k = (p_{k+1}-p_k)/p_k\n",
    "    # so if the asset has M points, we get M-1 returns\n",
    "    returns_i = []\n",
    "    for k in range(len(p) - 1):\n",
    "        if p[k] != 0:\n",
    "            ret_k = (p[k+1] - p[k]) / p[k]\n",
    "        else:\n",
    "            ret_k = 0.0\n",
    "        returns_i.append(ret_k)\n",
    "    if max_periods is None:\n",
    "        max_periods = len(returns_i)\n",
    "    else:\n",
    "        max_periods = min(max_periods, len(returns_i))\n",
    "    all_historical_returns.append(returns_i)\n",
    "\n",
    "# We should align them to the same number of time steps, if needed\n",
    "# to keep them consistent. We'll truncate to the smallest length among all assets.\n",
    "for i in range(n_assets):\n",
    "    all_historical_returns[i] = all_historical_returns[i][:max_periods]\n",
    "\n",
    "# Convert to array shape (max_periods, n_assets)\n",
    "all_historical_returns = np.array(all_historical_returns).T  # shape (max_periods, n_assets)\n",
    "\n",
    "print(\"all_historical_returns shape:\", all_historical_returns.shape)\n",
    "\n",
    "# Now compute sample mean and covariance of these historical returns.\n",
    "# But we won't use the sample mean. We have a forecast from time=100->200, predicted_returns.\n",
    "# We'll only use the covariance. So let's do:\n",
    "Sigma = np.cov(all_historical_returns, rowvar=False)  # shape (n_assets, n_assets)\n",
    "\n",
    "mu = predicted_returns  # from forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa4f5f3",
   "metadata": {},
   "source": [
    "Implement the two multi-objective methods:\n",
    "\t•\tWeighted Sum Method (WSM): For each \\alpha\\in[0,1], solve \\min \\alpha (x^T\\Sigma x) - (1-\\alpha) (\\mu^T x).\n",
    "\t•\tEpsilon-Constraint Method (ECM): For each target return r, solve \\min x^T\\Sigma x subject to \\mu^T x \\ge r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "33350e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# CELL 6: Implement WSM and ECM\n",
    "#   Weighted Sum Method (WSM):\n",
    "#     min alpha * x^T Sigma x  - (1 - alpha)*mu^T x\n",
    "#     s.t. sum(x_i)=1, x_i >= 0\n",
    "#   Epsilon Constraint Method (ECM):\n",
    "#     min x^T Sigma x\n",
    "#     s.t. mu^T x >= r\n",
    "#          sum(x_i)=1, x_i >= 0\n",
    "#\n",
    "# We'll define 2 functions that solve these for different alpha or r values.\n",
    "################################################################################\n",
    "\n",
    "def solve_qp_cvxopt(Q, c, A=None, b=None, G=None, h=None, show_progress=False):\n",
    "    \"\"\"\n",
    "    Minimizes (1/2)* x^T Q x + c^T x\n",
    "    s.t. Gx <= h\n",
    "         A x = b\n",
    "    \"\"\"\n",
    "    solvers.options['show_progress'] = show_progress\n",
    "\n",
    "    Q_cvx = matrix(Q, tc='d')\n",
    "    c_cvx = matrix(c, tc='d')\n",
    "    A_cvx = matrix(A, tc='d') if A is not None else None\n",
    "    b_cvx = matrix(b, tc='d') if b is not None else None\n",
    "    G_cvx = matrix(G, tc='d') if G is not None else None\n",
    "    h_cvx = matrix(h, tc='d') if h is not None else None\n",
    "\n",
    "    solution = solvers.qp(Q_cvx, c_cvx, G_cvx, h_cvx, A_cvx, b_cvx)\n",
    "    x_opt = np.array(solution['x']).flatten()\n",
    "    return x_opt, solution['primal objective']\n",
    "\n",
    "def wsm_portfolios(mu, Sigma, num_points=11):\n",
    "    \"\"\"\n",
    "    Weighted Sum Method\n",
    "    We'll vary alpha in [0,1] with 'num_points' steps\n",
    "    \"\"\"\n",
    "    n = len(mu)\n",
    "\n",
    "    # constraints: sum(x) = 1, x >= 0\n",
    "    A = np.ones((1, n))\n",
    "    b = np.array([1.0])\n",
    "    G = -1.0 * np.eye(n)\n",
    "    h = np.zeros(n)\n",
    "\n",
    "    alpha_list = np.linspace(0, 1, num_points)\n",
    "    results_risk = []\n",
    "    results_return = []\n",
    "    results_weights = []\n",
    "\n",
    "    for alpha in alpha_list:\n",
    "        # objective = alpha * x^T Sigma x - (1-alpha)* mu^T x\n",
    "        # in cvxopt form: 0.5 * x^T Q x + c^T x\n",
    "        # => Q = 2*alpha*Sigma  (so that 0.5 * Q = alpha*Sigma)\n",
    "        # => c = - (1-alpha)* mu\n",
    "        Q_eff = 2.0 * alpha * Sigma\n",
    "        c_eff = -(1.0 - alpha) * mu\n",
    "\n",
    "        x_opt, _ = solve_qp_cvxopt(Q_eff, c_eff, A=A, b=b, G=G, h=h)\n",
    "        # compute risk, return\n",
    "        risk_val = x_opt @ Sigma @ x_opt\n",
    "        ret_val = x_opt @ mu\n",
    "\n",
    "        results_risk.append(risk_val)\n",
    "        results_return.append(ret_val)\n",
    "        results_weights.append(x_opt)\n",
    "\n",
    "    return np.array(results_risk), np.array(results_return), np.array(results_weights)\n",
    "\n",
    "def ecm_portfolios(mu, Sigma, num_points=11):\n",
    "    \"\"\"\n",
    "    Epsilon Constraint Method\n",
    "    We pick r in [min(mu), max(mu)] with 'num_points' steps\n",
    "    Then solve:\n",
    "      min x^T Sigma x\n",
    "      s.t. mu^T x >= r, sum(x)=1, x>=0\n",
    "    \"\"\"\n",
    "    n = len(mu)\n",
    "    A = np.ones((1, n))\n",
    "    b = np.array([1.0])\n",
    "    base_G = -1.0 * np.eye(n)\n",
    "    base_h = np.zeros(n)\n",
    "\n",
    "    r_min, r_max = mu.min(), mu.max()\n",
    "    r_values = np.linspace(r_min, r_max, num_points)\n",
    "\n",
    "    results_risk = []\n",
    "    results_return = []\n",
    "    results_weights = []\n",
    "\n",
    "    for r in r_values:\n",
    "        # mu^T x >= r => -mu^T x <= -r\n",
    "        G_extended = np.vstack([base_G, -mu.reshape(1,-1)])\n",
    "        h_extended = np.concatenate([base_h, np.array([-r])])\n",
    "\n",
    "        # objective => x^T Sigma x => in cvxopt form => 0.5*x^T(2Sigma)x\n",
    "        Q_eff = 2.0 * Sigma\n",
    "        c_eff = np.zeros(n)\n",
    "\n",
    "        try:\n",
    "            x_opt, _ = solve_qp_cvxopt(Q_eff, c_eff, A=A, b=b, \n",
    "                                       G=G_extended, h=h_extended)\n",
    "            risk_val = x_opt @ Sigma @ x_opt\n",
    "            ret_val = x_opt @ mu\n",
    "\n",
    "            # accept only if ret_val >= r (numerical tolerance)\n",
    "            if ret_val >= r - 1e-6:\n",
    "                results_risk.append(risk_val)\n",
    "                results_return.append(ret_val)\n",
    "                results_weights.append(x_opt)\n",
    "        except:\n",
    "            # solver might fail or be infeasible\n",
    "            continue\n",
    "\n",
    "    return np.array(results_risk), np.array(results_return), np.array(results_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c062ac",
   "metadata": {},
   "source": [
    "\t•\tRun both WSM and ECM with different numbers of sample points (e.g., num_points=5,11,21,51).\n",
    "\t•\tPlot the resulting Pareto fronts.\n",
    "\t•\tCompare how many unique solutions each approach yields (some alphas can produce the same optimum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# CELL 7: Run & Compare For Different Calculation Limits, Plot Pareto Front\n",
    "################################################################################\n",
    "\n",
    "# Let's try a few different numbers of alpha steps for WSM\n",
    "points_list = [5, 11, 21, 51]  # e.g., 5, 11, 21, 51 points\n",
    "\n",
    "plt.figure()\n",
    "for num_points in points_list:\n",
    "    wsm_risk, wsm_ret, _ = wsm_portfolios(mu, Sigma, num_points=num_points)\n",
    "    plt.scatter(wsm_risk, wsm_ret, label=f\"WSM {num_points} pts\", alpha=0.7)\n",
    "\n",
    "# Also do the ECM approach with e.g. 11 points\n",
    "ecm_risk, ecm_ret, _ = ecm_portfolios(mu, Sigma, num_points=11)\n",
    "plt.scatter(ecm_risk, ecm_ret, marker='x', color='r', label=\"ECM 11 pts\")\n",
    "\n",
    "plt.xlabel(\"Portfolio Risk (Variance)\")\n",
    "plt.ylabel(\"Portfolio Return\")\n",
    "plt.title(\"Pareto Front: Weighted Sum & Epsilon-Constraint\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "################################################################################\n",
    "# We can also compare how many *unique* solutions each method returns.\n",
    "# E.g., for WSM with 51 alpha steps, do we get 51 unique solutions or fewer?\n",
    "################################################################################\n",
    "\n",
    "for num_points in points_list:\n",
    "    wsm_risk, wsm_ret, wsm_wts = wsm_portfolios(mu, Sigma, num_points=num_points)\n",
    "\n",
    "    # Round to mitigate floating precision issues\n",
    "    risk_ret_pairs = np.round(np.column_stack((wsm_risk, wsm_ret)), 6)\n",
    "    unique_pairs = np.unique(risk_ret_pairs, axis=0)\n",
    "    print(f\"[WSM] alpha steps = {num_points:2d} => found {len(unique_pairs)} unique (risk,return) solutions.\")\n",
    "\n",
    "ecm_risk, ecm_ret, ecm_wts = ecm_portfolios(mu, Sigma, num_points=11)\n",
    "risk_ret_pairs = np.round(np.column_stack((ecm_risk, ecm_ret)), 6)\n",
    "unique_ecm = np.unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25768f6c",
   "metadata": {},
   "source": [
    "## Interpretation of the Pareto Plots and Unique Solutions\n",
    "\n",
    "1. **Plot Description**  \n",
    "   - The *x-axis* represents the **portfolio risk** (variance). Lower values mean less risk.  \n",
    "   - The *y-axis* represents the **portfolio return** predicted by the model. Higher values mean higher return.  \n",
    "   - Each marker represents a different **optimal portfolio** found by the optimization method (Weighted Sum or Epsilon-Constraint).\n",
    "\n",
    "2. **Weighted Sum Method (WSM)**  \n",
    "   - We vary the parameter \\(\\alpha \\in [0,1]\\) in discrete steps (e.g., 5, 11, 21, 51).  \n",
    "   - For each \\(\\alpha\\), the model tries to minimize \\(\\alpha \\times \\text{risk} - (1-\\alpha)\\times \\text{return}\\).  \n",
    "   - The reported *unique* solutions are those with distinct \\((\\text{risk}, \\text{return})\\) pairs.  \n",
    "   - In the figure, you see fewer points when using fewer \\(\\alpha\\)-steps because multiple \\(\\alpha\\) values can produce the *same* portfolio.  \n",
    "   - As you increase the number of \\(\\alpha\\)-steps, you typically discover more unique points on the Pareto front.  \n",
    "\n",
    "3. **Epsilon-Constraint Method (ECM)**  \n",
    "   - We vary a target return \\(r\\) and minimize \\(\\text{risk}\\) subject to \\(\\text{return} \\ge r\\).  \n",
    "   - The red “x” markers in the plot show the ECM solutions for 11 target-return steps.  \n",
    "   - Notice these solutions can span a somewhat different shape or distribution than WSM because the Pareto frontier is sampled by fixing \\(\\text{return}\\) rather than weighting risk/return directly.\n",
    "\n",
    "4. **Observations & Conclusions**  \n",
    "   - Both methods identify *efficient (Pareto-optimal) portfolios*: each point is a valid trade-off between risk and return.  \n",
    "   - **WSM solutions** can “bunch up” in some regions if those risk/return objectives lead to the same optimum. Increasing the number of \\(\\alpha\\) steps from 5 to 51 yields more unique portfolios.  \n",
    "   - **ECM solutions** for 11 steps may yield a different coverage of the front.  \n",
    "   - Comparing the two sets of solutions shows that the Pareto front has a characteristic shape where risk increases to achieve higher returns.  \n",
    "   - Generally, you can see which **method** (and how many sampling steps) is more effective in covering the front thoroughly.  \n",
    "\n",
    "5. **Next Steps**  \n",
    "   - If you need a denser approximation, increase the number of steps (either \\(\\alpha\\)-steps for WSM or return-target steps for ECM).  \n",
    "   - If you see identical solutions repeated, that indicates multiple parameter values converge to the same optimum.  \n",
    "   - You can also combine these two approaches or adopt more sophisticated sampling methods (e.g., binary search on returns, or multi-objective evolutionary algorithms) to capture the frontier more completely."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
